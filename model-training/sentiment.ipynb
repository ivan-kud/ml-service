{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76dd1590-1dd7-4380-93b7-4491789eb84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport sentiment_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff51efb6-9982-433e-b9b7-73a3ad88d354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python:          3.10.11 (main, Apr  7 2023, 07:24:53) [Clang 14.0.0 (clang-1400.0.29.202)]\n",
      "scikit-learn:    1.2.2\n",
      "Gensim:          4.3.1\n",
      "PyTorch:         1.13.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import datasets\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, HashingVectorizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print('python:'.ljust(16), sys.version.split('\\n')[0])\n",
    "print('scikit-learn:'.ljust(16), sklearn.__version__)\n",
    "print('Gensim:'.ljust(16), gensim.__version__)\n",
    "print('PyTorch:'.ljust(16), torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d3a80-84d9-4a1a-996a-bc30784d09db",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5038b4-d83c-4bde-a230-0ca7538d5b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {DEVICE} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d1d08d-6b4f-4979-8ff5-b397c84c0383",
   "metadata": {},
   "source": [
    "# Hyperparameters & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6420d850-2109-4edc-937f-246d5d0312cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# BATCH_SIZE = 64\n",
    "# EPOCHS = 15  # select from: 2**n - 1 = [1, 3, 7, 15, ...]\n",
    "# SCHEDULER_GAMMA = 0.7\n",
    "\n",
    "# Constants\n",
    "WORKING_PATH = './sentiment-data/'\n",
    "MODEL_PATH = '../app/models/'\n",
    "DATASET_NAME = 'tweet_eval'\n",
    "DATASET_CONF = 'sentiment'\n",
    "CLASSES = 3\n",
    "HUGGINGFACE_MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "\n",
    "# Actions\n",
    "# DO_LR_RANGE_TEST=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59c5513-1785-4de5-a58b-797d6ddd243e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a433eb7-c8d0-44bf-a7d1-867166772870",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2147483647\n",
    "# random.seed(RANDOM_STATE)\n",
    "# np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed_all(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a62e08-a57d-46f5-8d21-74df9a240575",
   "metadata": {},
   "source": [
    "# Load & show data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbc60f77-c4fe-459c-9a84-e2c674833aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tweet_eval (/Users/admin/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba4b3fc1a4f45118d3a625b7a7420e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 45615\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12284\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(DATASET_NAME, DATASET_CONF)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a608d6b-b412-4319-8576-02575aa5d512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"',\n",
       "  '\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"',\n",
       "  'Sorry bout the stream last night I crashed out but will be on tonight for sure. Then back to Minecraft in pc tomorrow night.',\n",
       "  \"Chase Headley's RBI double in the 8th inning off David Price snapped a Yankees streak of 33 consecutive scoreless innings against Blue Jays\",\n",
       "  '@user Alciato: Bee will invest 150 million in January, another 200 in the Summer and plans to bring Messi by 2017\"'],\n",
       " 'label': [2, 1, 1, 1, 2]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96a266-7271-4ae2-816c-a5bdf86037b3",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3040eda8-5742-4c0d-b412-343ad7dd3fee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"\n",
      "\" qt @user origin draft 7th book , remu lupin surviv battl hogwarts. #happybirthdayremuslupin \"\n",
      "\n",
      "\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"\n",
      "\" ben smith / smith ( concuss ) remain lineup thursday , curti #nhl #sj \"\n",
      "\n",
      "Sorry bout the stream last night I crashed out but will be on tonight for sure. Then back to Minecraft in pc tomorrow night.\n",
      "sorri bout stream last night crash tonight sure. back minecraft pc tomorrow night .\n",
      "\n",
      "Chase Headley's RBI double in the 8th inning off David Price snapped a Yankees streak of 33 consecutive scoreless innings against Blue Jays\n",
      "chase headley ' rbi doubl 8th inning david price snap yanke streak 33 consecut scoreless inning blue jay\n",
      "\n",
      "@user Alciato: Bee will invest 150 million in January, another 200 in the Summer and plans to bring Messi by 2017\"\n",
      "@user alciato : bee invest 150 million januari , anoth 200 summer plan bring messi 2017 \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = sentiment_utils.Tokenizer()\n",
    "\n",
    "# Print tokenization examples\n",
    "for text in dataset['train']['text'][:5]:\n",
    "    print(text)\n",
    "    print(tokenizer(text, return_str=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29efae-6858-4dd6-98d4-7f0824f53130",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "## Document vectorizers\n",
    "### BOW, TF-IDF, Hashing BOW and their SVD variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a363196-375b-4a06-a9c2-c648e0ce46f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full size data shapes:\n",
      "BOW:         (45615, 50000) (2000, 50000) (12284, 50000)\n",
      "TF-IDF:      (45615, 50000) (2000, 50000) (12284, 50000)\n",
      "Hashing BOW: (45615, 50000) (2000, 50000) (12284, 50000)\n",
      "\n",
      "SVD-truncated data shapes:\n",
      "BOW:         (45615, 100) (2000, 100) (12284, 100)\n",
      "TF-IDF:      (45615, 100) (2000, 100) (12284, 100)\n",
      "Hashing BOW: (45615, 100) (2000, 100) (12284, 100)\n",
      "\n",
      "Explained variance for SVD:\n",
      "BOW:         51.53 %\n",
      "TF-IDF:      16.50 %\n",
      "Hashing BOW: 41.75 %\n",
      "\n",
      "CPU times: user 24.7 ms, sys: 97.8 ms, total: 122 ms\n",
      "Wall time: 187 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_features = 50000\n",
    "svd_components = 100\n",
    "file = WORKING_PATH + 'bow_tfidf_' + str(n_features) + '_' + str(svd_components) + '.pickle'\n",
    "saving = True\n",
    "\n",
    "# Load vectorizer if it already exists\n",
    "if os.path.isfile(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        output = pickle.load(f)\n",
    "        \n",
    "    (\n",
    "        bow_vectorizer, x_train_bow, x_valid_bow, x_test_bow,\n",
    "        tfidf_vectorizer, x_train_tfidf, x_valid_tfidf, x_test_tfidf,\n",
    "        hashing_vectorizer, x_train_hashing, x_valid_hashing, x_test_hashing,\n",
    "        svd_bow_vectorizer, x_train_svd_bow, x_valid_svd_bow, x_test_svd_bow,\n",
    "        svd_tfidf_vectorizer, x_train_svd_tfidf, x_valid_svd_tfidf, x_test_svd_tfidf,\n",
    "        svd_hashing_vectorizer, x_train_svd_hashing, x_valid_svd_hashing, x_test_svd_hashing,\n",
    "    ) = output\n",
    "\n",
    "else:\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    # Initialize vectorizers\n",
    "    bow_vectorizer = CountVectorizer(lowercase=False,\n",
    "                                     tokenizer=tokenizer,\n",
    "                                     max_features=n_features)\n",
    "    tfidf_vectorizer = TfidfTransformer()\n",
    "    hashing_vectorizer = HashingVectorizer(lowercase=False,\n",
    "                                           tokenizer=tokenizer,\n",
    "                                           n_features=n_features)\n",
    "\n",
    "    # Initialize SVD-truncated vectorizers\n",
    "    svd_bow_vectorizer = TruncatedSVD(n_components=svd_components)\n",
    "    svd_tfidf_vectorizer = TruncatedSVD(n_components=svd_components)\n",
    "    svd_hashing_vectorizer = TruncatedSVD(n_components=svd_components)\n",
    "\n",
    "    # Fit vectorizers and transform train data\n",
    "    x_train_bow = bow_vectorizer.fit_transform(dataset['train']['text'])\n",
    "    x_train_tfidf = tfidf_vectorizer.fit_transform(x_train_bow)\n",
    "    x_train_hashing = hashing_vectorizer.fit_transform(dataset['train']['text'])\n",
    "\n",
    "    # Fit SVD-truncated vectorizers and transform train data\n",
    "    x_train_svd_bow = svd_bow_vectorizer.fit_transform(x_train_bow)\n",
    "    x_train_svd_tfidf = svd_tfidf_vectorizer.fit_transform(x_train_tfidf)\n",
    "    x_train_svd_hashing = svd_hashing_vectorizer.fit_transform(x_train_hashing)\n",
    "\n",
    "    # Transform validation and test data\n",
    "    x_valid_bow = bow_vectorizer.transform(dataset['validation']['text'])\n",
    "    x_valid_tfidf = tfidf_vectorizer.transform(x_valid_bow)\n",
    "    x_valid_hashing = hashing_vectorizer.transform(dataset['validation']['text'])\n",
    "    x_test_bow = bow_vectorizer.transform(dataset['test']['text'])\n",
    "    x_test_tfidf = tfidf_vectorizer.transform(x_test_bow)\n",
    "    x_test_hashing = hashing_vectorizer.transform(dataset['test']['text'])\n",
    "\n",
    "    # Transform validation and test data for SVD-truncated vectorizers\n",
    "    x_valid_svd_bow = svd_bow_vectorizer.transform(x_valid_bow)\n",
    "    x_valid_svd_tfidf = svd_tfidf_vectorizer.transform(x_valid_tfidf)\n",
    "    x_valid_svd_hashing = svd_hashing_vectorizer.transform(x_valid_hashing)\n",
    "    x_test_svd_bow = svd_bow_vectorizer.transform(x_test_bow)\n",
    "    x_test_svd_tfidf = svd_tfidf_vectorizer.transform(x_test_tfidf)\n",
    "    x_test_svd_hashing = svd_hashing_vectorizer.transform(x_test_hashing)\n",
    "    \n",
    "    # Form output\n",
    "    output = (\n",
    "        bow_vectorizer, x_train_bow, x_valid_bow, x_test_bow,\n",
    "        tfidf_vectorizer, x_train_tfidf, x_valid_tfidf, x_test_tfidf,\n",
    "        hashing_vectorizer, x_train_hashing, x_valid_hashing, x_test_hashing,\n",
    "        svd_bow_vectorizer, x_train_svd_bow, x_valid_svd_bow, x_test_svd_bow,\n",
    "        svd_tfidf_vectorizer, x_train_svd_tfidf, x_valid_svd_tfidf, x_test_svd_tfidf,\n",
    "        svd_hashing_vectorizer, x_train_svd_hashing, x_valid_svd_hashing, x_test_svd_hashing,\n",
    "    )\n",
    "\n",
    "    # Save vectorizers and transformed data\n",
    "    if saving:\n",
    "        with open(file, 'wb') as f:\n",
    "            pickle.dump(output, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Print shapes\n",
    "print('Full size data shapes:')\n",
    "print('BOW:        ', x_train_bow.shape, x_valid_bow.shape, x_test_bow.shape)\n",
    "print('TF-IDF:     ', x_train_tfidf.shape, x_valid_tfidf.shape, x_test_tfidf.shape)\n",
    "print('Hashing BOW:', x_train_hashing.shape, x_valid_hashing.shape, x_test_hashing.shape)\n",
    "print()\n",
    "print('SVD-truncated data shapes:')\n",
    "print('BOW:        ', x_train_svd_bow.shape, x_valid_svd_bow.shape, x_test_svd_bow.shape)\n",
    "print('TF-IDF:     ', x_train_svd_tfidf.shape, x_valid_svd_tfidf.shape, x_test_svd_tfidf.shape)\n",
    "print('Hashing BOW:', x_train_svd_hashing.shape, x_valid_svd_hashing.shape, x_test_svd_hashing.shape)\n",
    "print()\n",
    "\n",
    "# Print SVD explained variance\n",
    "print('Explained variance for SVD:')\n",
    "print(f'BOW:         {(svd_bow_vectorizer.explained_variance_ratio_.sum() * 100):.2f} %')\n",
    "print(f'TF-IDF:      {(svd_tfidf_vectorizer.explained_variance_ratio_.sum() * 100):.2f} %')\n",
    "print(f'Hashing BOW: {(svd_hashing_vectorizer.explained_variance_ratio_.sum() * 100):.2f} %')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3252149e-5ec6-43c3-9407-036cb06ab72a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document vector examples and their shapes:\n",
      "\n",
      "BOW:\n",
      "[[0 2 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]]\n",
      "(2, 50000)\n",
      "\n",
      "TF-IDF:\n",
      "[[0.         0.1592305  0.         ... 0.         0.         0.        ]\n",
      " [0.         0.16949097 0.         ... 0.         0.         0.        ]]\n",
      "(2, 50000)\n",
      "\n",
      "Hashing BOW:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(2, 50000)\n",
      "\n",
      "SVD BOW:\n",
      "[ 2.19219037  0.02814938 -0.37149003  0.46869895  0.77413124 -0.27684479]\n",
      "(2, 100)\n",
      "\n",
      "SVD TF-IDF:\n",
      "[ 0.16403713 -0.06192932  0.00118506  0.05926234 -0.01728777 -0.0282153 ]\n",
      "(2, 100)\n",
      "\n",
      "SVD Hashing BOW:\n",
      "[ 0.51587004 -0.1754948   0.05378128  0.20697629 -0.04919744 -0.03881921]\n",
      "(2, 100)\n"
     ]
    }
   ],
   "source": [
    "example = dataset['train']['text'][:2]\n",
    "\n",
    "# Vecotrize document example\n",
    "bow_output = bow_vectorizer.transform(example)\n",
    "tfidf_output = tfidf_vectorizer.transform(bow_output)\n",
    "hashing_output = hashing_vectorizer.transform(example)\n",
    "svd_bow_output = svd_bow_vectorizer.transform(bow_output)\n",
    "svd_tfidf_output = svd_tfidf_vectorizer.transform(tfidf_output)\n",
    "svd_hashing_output = svd_hashing_vectorizer.transform(hashing_output)\n",
    "\n",
    "# Print document vectors and their shapes\n",
    "print('Document vector examples and their shapes:')\n",
    "print()\n",
    "print('BOW:')\n",
    "print(bow_output.todense())\n",
    "print(bow_output.shape)\n",
    "print()\n",
    "print('TF-IDF:')\n",
    "print(tfidf_output.todense())\n",
    "print(tfidf_output.shape)\n",
    "print()\n",
    "print('Hashing BOW:')\n",
    "print(hashing_output.todense())\n",
    "print(hashing_output.shape)\n",
    "print()\n",
    "print('SVD BOW:')\n",
    "print(svd_bow_output[0][:6])\n",
    "print(svd_bow_output.shape)\n",
    "print()\n",
    "print('SVD TF-IDF:')\n",
    "print(svd_tfidf_output[0][:6])\n",
    "print(svd_tfidf_output.shape)\n",
    "print()\n",
    "print('SVD Hashing BOW:')\n",
    "print(svd_hashing_output[0][:6])\n",
    "print(svd_hashing_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51485a-e469-4dba-8e1f-e49f62abde40",
   "metadata": {},
   "source": [
    "## Token vectorizers\n",
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beb38de3-5e15-401c-8e73-30b1e2662e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary shape:\n",
      "(8865, 100)\n",
      "\n",
      "Most frequent words:\n",
      "\"\n",
      "'\n",
      ",\n",
      "@user\n",
      "!\n",
      ".\n",
      ":\n",
      "...\n",
      "may\n",
      "tomorrow\n",
      "?\n",
      "go\n",
      "day\n",
      ")\n",
      "see\n",
      "get\n",
      ";\n",
      "night\n",
      "-\n",
      "(\n",
      "\n",
      "CPU times: user 49.6 ms, sys: 5.1 ms, total: 54.7 ms\n",
      "Wall time: 54.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vector_size = 100\n",
    "file = WORKING_PATH + 'word2vec_' + str(vector_size) + '.gensim'\n",
    "saving = True\n",
    "\n",
    "# Load model if it already exists\n",
    "if os.path.isfile(file):\n",
    "    word2vec = gensim.models.Word2Vec.load(file)\n",
    "\n",
    "else:\n",
    "    # Initialize tokenizer wiht corpus in it\n",
    "    tokenizer = sentiment_utils.Tokenizer()\n",
    "    tokenizer.corpus = dataset['train']['text']\n",
    "\n",
    "    # Train the model\n",
    "    word2vec = gensim.models.Word2Vec(\n",
    "        sentences=tokenizer, min_count=5, vector_size=vector_size, sg=1, hs=0, negative=5,\n",
    "        workers=7, epochs=5, seed=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    if saving:\n",
    "        word2vec.save(file)\n",
    "\n",
    "# Print vocabulary shape\n",
    "print('Vocabulary shape:')\n",
    "print((len(word2vec.wv.index_to_key), vector_size))\n",
    "print()\n",
    "\n",
    "# Print most frequent words\n",
    "print('Most frequent words:')\n",
    "for word in word2vec.wv.index_to_key[:20]:\n",
    "    print(word)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b4a724d-eae0-4972-9e91-362ce7271af4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A vector example:\n",
      "[-1.7856718e-04 -7.1627498e-03  5.6681838e-03  3.3616947e-03\n",
      " -4.3037655e-03 -9.9052377e-03  7.8344531e-03 -3.0744076e-03\n",
      " -4.9395845e-03  2.8602481e-03  3.4456314e-03  7.0434297e-03\n",
      "  4.8328913e-03 -2.2578656e-03  9.6133817e-03  6.0931980e-03\n",
      " -6.9454909e-05 -7.1364711e-03 -4.3063499e-03 -4.0867887e-03\n",
      "  7.2207870e-03  1.9087791e-04  8.7932190e-03  3.0965197e-03\n",
      "  8.6710928e-03 -1.7151153e-03 -3.0472302e-03  2.3653186e-03\n",
      " -1.0201788e-03 -1.1647308e-03  6.5529346e-04  8.5652145e-03\n",
      "  6.0522771e-03  7.2767651e-03 -1.3941026e-03 -3.9289714e-04\n",
      "  8.7667480e-03  3.2828213e-04  7.9442039e-03 -5.4157972e-03\n",
      " -1.1028457e-03  8.5191736e-03  5.3559565e-03 -2.3469485e-03\n",
      "  1.3705313e-03 -8.5036922e-03 -1.4750564e-03 -1.0823035e-03\n",
      "  9.5225284e-03  7.1474756e-03 -9.3324063e-04  3.3289576e-03\n",
      "  4.4522537e-03  1.0433340e-03  9.0771206e-03  3.6466957e-04\n",
      " -3.4984981e-03  9.3069943e-03  9.0731289e-03  2.9107772e-03\n",
      " -2.8863156e-03 -8.8340221e-03  4.6982062e-03  2.8851735e-03\n",
      "  5.5834125e-03 -8.0729220e-03  6.2071667e-03 -8.5791489e-03\n",
      " -8.5380506e-03 -4.9521209e-04  9.4182463e-03 -9.7873006e-03\n",
      "  1.9779515e-03  7.0411875e-03  2.7419114e-03  2.2641600e-03\n",
      " -7.8972280e-03 -2.0413399e-05  3.6206401e-03  2.7533746e-03\n",
      "  1.9868994e-03  5.3465436e-03  1.1230338e-03  2.8817214e-03\n",
      "  8.7525090e-03 -1.2829840e-03 -2.3385524e-04  7.4848901e-03\n",
      "  4.8601939e-03 -4.7724936e-03  3.4836209e-03 -3.9947272e-04\n",
      "  8.7455921e-03  9.6359225e-03  9.3975803e-03  3.5232091e-03\n",
      " -5.5122506e-03  1.9586694e-03  2.4390114e-03 -1.1434662e-03]\n"
     ]
    }
   ],
   "source": [
    "# Print the vector example\n",
    "print('A vector example:')\n",
    "print(word2vec.wv['@user'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6362e-5bcb-4ae9-8739-6a7ef29efc29",
   "metadata": {},
   "source": [
    "### RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed5173ee-4af2-4dbb-87b8-e99ff4fd413e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"\n",
      "tensor([[    0,   113,  1864,   565,   787, 12105,    96,     5,  1461,  2479,\n",
      "             9,     5,   262,   212,  1040,     6,  8022,   687, 26110,   179,\n",
      "          5601,     5,  9846,     9, 42210,     4,   849, 21136, 44728,  1208,\n",
      "         31157,   687,   574,   658,   179,   113,    22,  1864,   565,   787,\n",
      "         12105,    96,     5,  1461,  2479,     9,     5,   262,   212,  1040,\n",
      "             6,  8022,   687, 26110,   179,  5601,     5,  9846,     9, 42210,\n",
      "             4,   849, 21136, 44728,  1208, 31157,   687,   574,   658,   179,\n",
      "           113,     2]])\n",
      "\n",
      "\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"\n",
      "tensor([[    0,   113, 17521,  1259,  1589,  1259,    36,  3865, 33825,    43,\n",
      "          1189,    66,     9,     5,  4451,   296,     6, 11292,   849,   487,\n",
      "          8064,   849,   104,   863,   113,    22, 17521,  1259,  1589,  1259,\n",
      "            36,  3865, 33825,    43,  1189,    66,     9,     5,  4451,   296,\n",
      "             6, 11292,   849,   487,  8064,   849,   104,   863,   113,     2]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Arbitrary length vectorizers (ALV)\n",
    "alv_pretrained = AutoTokenizer.from_pretrained(HUGGINGFACE_MODEL_NAME)\n",
    "\n",
    "for text in dataset['train']['text'][:2]:\n",
    "    print(text)\n",
    "    preprocessed_text = sentiment_utils.preprocess_text(text)\n",
    "    print(alv_pretrained(preprocessed_text, return_tensors='pt')['input_ids'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5994ec64-fd12-4eab-83a1-180fc2c7c8fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(alv_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "980647cb-223f-4652-b443-7ff76cb51afc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.98 s, sys: 125 ms, total: 9.1 s\n",
      "Wall time: 2.13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 45615\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 12284\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# To process the whole corpus\n",
    "def preprocess_function(examples):\n",
    "    preprocessed_text = sentiment_utils.preprocess_text(examples['text'])\n",
    "    return alv_pretrained(preprocessed_text)\n",
    "\n",
    "dataset_alv = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "dataset_alv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a337f-3a29-4f79-a798-320296889f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
