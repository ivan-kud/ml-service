{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76dd1590-1dd7-4380-93b7-4491789eb84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport sentiment_utils\n",
    "%aimport mytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff51efb6-9982-433e-b9b7-73a3ad88d354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python:          3.11.3 (main, Apr  7 2023, 20:13:31) [Clang 14.0.0 (clang-1400.0.29.202)]\n",
      "scikit-learn:    1.2.2\n",
      "Gensim:          4.3.1\n",
      "PyTorch:         2.0.1\n",
      "Transformers:    4.29.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import datasets\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, HashingVectorizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_lr_finder import LRFinder\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print('python:'.ljust(16), sys.version.split('\\n')[0])\n",
    "print('scikit-learn:'.ljust(16), sklearn.__version__)\n",
    "print('Gensim:'.ljust(16), gensim.__version__)\n",
    "print('PyTorch:'.ljust(16), torch.__version__)\n",
    "print('Transformers:'.ljust(16), transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d3a80-84d9-4a1a-996a-bc30784d09db",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5038b4-d83c-4bde-a230-0ca7538d5b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {DEVICE} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d1d08d-6b4f-4979-8ff5-b397c84c0383",
   "metadata": {},
   "source": [
    "# Hyperparameters & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6420d850-2109-4edc-937f-246d5d0312cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "VOCAB_SIZE = 50000\n",
    "SVD_SIZE = 100\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15  # select from: 2**n - 1 = [1, 3, 7, 15, ...]\n",
    "SCHEDULER_GAMMA = 0.7\n",
    "\n",
    "# Constants\n",
    "WORKING_PATH = './sentiment-data/'\n",
    "MODEL_PATH = '../app/models/'\n",
    "DATASET_NAME = 'tweet_eval'\n",
    "DATASET_CONF = 'sentiment'\n",
    "CLASSES = 3\n",
    "LABEL_MAP = {\n",
    "    0: 'negative',\n",
    "    1: 'neutral',\n",
    "    2: 'positive',\n",
    "}\n",
    "HUGGINGFACE_MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "\n",
    "# Actions\n",
    "DO_LR_RANGE_TEST=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59c5513-1785-4de5-a58b-797d6ddd243e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a433eb7-c8d0-44bf-a7d1-867166772870",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2147483647\n",
    "# random.seed(RANDOM_STATE)\n",
    "# np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed_all(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a62e08-a57d-46f5-8d21-74df9a240575",
   "metadata": {},
   "source": [
    "# Load & show data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbc60f77-c4fe-459c-9a84-e2c674833aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tweet_eval (/Users/admin/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baedf4500a054bd18219102c01030fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 45615\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12284\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(DATASET_NAME, DATASET_CONF)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a608d6b-b412-4319-8576-02575aa5d512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"',\n",
       "  '\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"',\n",
       "  'Sorry bout the stream last night I crashed out but will be on tonight for sure. Then back to Minecraft in pc tomorrow night.',\n",
       "  \"Chase Headley's RBI double in the 8th inning off David Price snapped a Yankees streak of 33 consecutive scoreless innings against Blue Jays\",\n",
       "  '@user Alciato: Bee will invest 150 million in January, another 200 in the Summer and plans to bring Messi by 2017\"'],\n",
       " 'label': [2, 1, 1, 1, 2]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96a266-7271-4ae2-816c-a5bdf86037b3",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "## TokTokTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3040eda8-5742-4c0d-b412-343ad7dd3fee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"\n",
      "\" qt @user origin draft 7th book , remu lupin surviv battl hogwarts. #happybirthdayremuslupin \"\n",
      "\n",
      "\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"\n",
      "\" ben smith / smith ( concuss ) remain lineup thursday , curti #nhl #sj \"\n",
      "\n",
      "Sorry bout the stream last night I crashed out but will be on tonight for sure. Then back to Minecraft in pc tomorrow night.\n",
      "sorri bout stream last night crash tonight sure. back minecraft pc tomorrow night .\n",
      "\n",
      "Chase Headley's RBI double in the 8th inning off David Price snapped a Yankees streak of 33 consecutive scoreless innings against Blue Jays\n",
      "chase headley ' rbi doubl 8th inning david price snap yanke streak 33 consecut scoreless inning blue jay\n",
      "\n",
      "@user Alciato: Bee will invest 150 million in January, another 200 in the Summer and plans to bring Messi by 2017\"\n",
      "@user alciato : bee invest 150 million januari , anoth 200 summer plan bring messi 2017 \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = sentiment_utils.Tokenizer()\n",
    "\n",
    "# Print tokenization examples\n",
    "for text in dataset['train']['text'][:5]:\n",
    "    print(text)\n",
    "    print(tokenizer(text, return_str=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1283f037-250c-4c24-b8e1-4a6fd5a4bfed",
   "metadata": {},
   "source": [
    "## RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed5173ee-4af2-4dbb-87b8-e99ff4fd413e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"\n",
      "tensor([[    0,   113,  1864,   565,   787, 12105,    96,     5,  1461,  2479,\n",
      "             9,     5,   262,   212,  1040,     6,  8022,   687, 26110,   179,\n",
      "          5601,     5,  9846,     9, 42210,     4,   849, 21136, 44728,  1208,\n",
      "         31157,   687,   574,   658,   179,   113,    22,  1864,   565,   787,\n",
      "         12105,    96,     5,  1461,  2479,     9,     5,   262,   212,  1040,\n",
      "             6,  8022,   687, 26110,   179,  5601,     5,  9846,     9, 42210,\n",
      "             4,   849, 21136, 44728,  1208, 31157,   687,   574,   658,   179,\n",
      "           113,     2]])\n",
      "\n",
      "\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"\n",
      "tensor([[    0,   113, 17521,  1259,  1589,  1259,    36,  3865, 33825,    43,\n",
      "          1189,    66,     9,     5,  4451,   296,     6, 11292,   849,   487,\n",
      "          8064,   849,   104,   863,   113,    22, 17521,  1259,  1589,  1259,\n",
      "            36,  3865, 33825,    43,  1189,    66,     9,     5,  4451,   296,\n",
      "             6, 11292,   849,   487,  8064,   849,   104,   863,   113,     2]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(HUGGINGFACE_MODEL_NAME)\n",
    "\n",
    "for text in dataset['train']['text'][:2]:\n",
    "    print(text)\n",
    "    preprocessed_text = sentiment_utils.preprocess_text(text)\n",
    "    print(tokenizer(preprocessed_text, return_tensors='pt')['input_ids'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "980647cb-223f-4652-b443-7ff76cb51afc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # To process the whole corpus\n",
    "# def preprocess_function(examples):\n",
    "#     preprocessed_text = sentiment_utils.preprocess_text(examples['text'])\n",
    "#     return tokenizer(preprocessed_text)\n",
    "\n",
    "# dataset_alv = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# dataset_alv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29efae-6858-4dd6-98d4-7f0824f53130",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "## Document vectorizers\n",
    "### BOW, TF-IDF, Hashing BOW and their SVD variants\n",
    "Fit vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a363196-375b-4a06-a9c2-c648e0ce46f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/Documents/GitHub/ml-service/venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance for SVD:\n",
      "BOW:         51.53 %\n",
      "Hashing BOW: 41.75 %\n",
      "TF-IDF:      16.51 %\n",
      "\n",
      "CPU times: user 1min 10s, sys: 7.23 s, total: 1min 17s\n",
      "Wall time: 20.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_vectorizers = True\n",
    "file_vectorizers = WORKING_PATH + 'document_vectorizers_' + str(VOCAB_SIZE) + '_' + str(SVD_SIZE) + '.pickle'\n",
    "\n",
    "# Load vectorizer if it already exists\n",
    "if os.path.isfile(file_vectorizers):\n",
    "    with open(file_vectorizers, 'rb') as f:\n",
    "        vectorizers = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = sentiment_utils.Tokenizer()\n",
    "\n",
    "    # Initialize vectorizers\n",
    "    vectorizers = {'toktok': {\n",
    "        'bow': CountVectorizer(lowercase=False,\n",
    "                               tokenizer=tokenizer,\n",
    "                               max_features=VOCAB_SIZE),\n",
    "        'hbow': HashingVectorizer(lowercase=False,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  n_features=VOCAB_SIZE),\n",
    "        'tfidf': TfidfTransformer(),\n",
    "        'bow_svd': TruncatedSVD(n_components=SVD_SIZE),\n",
    "        'hbow_svd': TruncatedSVD(n_components=SVD_SIZE),\n",
    "        'tfidf_svd': TruncatedSVD(n_components=SVD_SIZE),\n",
    "    }}\n",
    "    # Fit vectorizers and transform train data\n",
    "    bow_train_texts = vectorizers['toktok']['bow'].fit_transform(dataset['train']['text'])\n",
    "    hbow_train_texts = vectorizers['toktok']['hbow'].fit_transform(dataset['train']['text'])\n",
    "    tfidf_train_texts = vectorizers['toktok']['tfidf'].fit_transform(bow_train_texts)\n",
    "\n",
    "    # Fit SVD-truncated vectorizers\n",
    "    vectorizers['toktok']['bow_svd'].fit(bow_train_texts)\n",
    "    vectorizers['toktok']['hbow_svd'].fit(hbow_train_texts)\n",
    "    vectorizers['toktok']['tfidf_svd'].fit(tfidf_train_texts)\n",
    "    \n",
    "    # Save vectorizers\n",
    "    if save_vectorizers:\n",
    "        with open(file_vectorizers, 'wb') as f:\n",
    "            pickle.dump(vectorizers, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Print SVD explained variance\n",
    "print('Explained variance for SVD:')\n",
    "print('BOW:        ', round(vectorizers['toktok']['bow_svd'].explained_variance_ratio_.sum() * 100, 2), '%')\n",
    "print('Hashing BOW:', round(vectorizers['toktok']['hbow_svd'].explained_variance_ratio_.sum() * 100, 2), '%')\n",
    "print('TF-IDF:     ', round(vectorizers['toktok']['tfidf_svd'].explained_variance_ratio_.sum() * 100, 2), '%')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21281e38-f9d6-40b4-94db-fd66c1bf3165",
   "metadata": {},
   "source": [
    "Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aded4063-7e5f-4fe7-8689-54262cfc545b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full size data shapes:\n",
      "BOW:         (45615, 50000) (2000, 50000) (12284, 50000)\n",
      "Hashing BOW: (45615, 50000) (2000, 50000) (12284, 50000)\n",
      "TF-IDF:      (45615, 50000) (2000, 50000) (12284, 50000)\n",
      "\n",
      "SVD-truncated data shapes:\n",
      "BOW:         (45615, 100) (2000, 100) (12284, 100)\n",
      "Hashing BOW: (45615, 100) (2000, 100) (12284, 100)\n",
      "TF-IDF:      (45615, 100) (2000, 100) (12284, 100)\n",
      "\n",
      "CPU times: user 14 s, sys: 88.2 ms, total: 14.1 s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transform_data = True\n",
    "save_data = True\n",
    "file_data = WORKING_PATH + 'document_data_' + str(VOCAB_SIZE) + '_' + str(SVD_SIZE) + '.pickle'\n",
    "\n",
    "if transform_data:\n",
    "    # Load transformed data if it already exists\n",
    "    if os.path.isfile(file_data):\n",
    "        with open(file_data, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "    else:\n",
    "        data = {'toktok': {\n",
    "            'bow': {\n",
    "                'train': vectorizers['toktok']['bow'].transform(dataset['train']['text']),\n",
    "                'valid': vectorizers['toktok']['bow'].transform(dataset['validation']['text']),\n",
    "                'test': vectorizers['toktok']['bow'].transform(dataset['test']['text']),\n",
    "            },\n",
    "            'hbow': {\n",
    "                'train': vectorizers['toktok']['hbow'].transform(dataset['train']['text']),\n",
    "                'valid': vectorizers['toktok']['hbow'].transform(dataset['validation']['text']),\n",
    "                'test': vectorizers['toktok']['hbow'].transform(dataset['test']['text']),\n",
    "            },\n",
    "        }}\n",
    "        \n",
    "        data['toktok']['tfidf'] = {\n",
    "            'train': vectorizers['toktok']['tfidf'].transform(data['toktok']['bow']['train']),\n",
    "            'valid': vectorizers['toktok']['tfidf'].transform(data['toktok']['bow']['valid']),\n",
    "            'test': vectorizers['toktok']['tfidf'].transform(data['toktok']['bow']['test']),\n",
    "        }\n",
    "\n",
    "        data['toktok']['bow_svd'] = {\n",
    "            'train': vectorizers['toktok']['bow_svd'].transform(data['toktok']['bow']['train']),\n",
    "            'valid': vectorizers['toktok']['bow_svd'].transform(data['toktok']['bow']['valid']),\n",
    "            'test': vectorizers['toktok']['bow_svd'].transform(data['toktok']['bow']['test']),\n",
    "        }\n",
    "\n",
    "        data['toktok']['hbow_svd'] = {\n",
    "            'train': vectorizers['toktok']['hbow_svd'].transform(data['toktok']['hbow']['train']),\n",
    "            'valid': vectorizers['toktok']['hbow_svd'].transform(data['toktok']['hbow']['valid']),\n",
    "            'test': vectorizers['toktok']['hbow_svd'].transform(data['toktok']['hbow']['test']),\n",
    "        }\n",
    "\n",
    "        data['toktok']['tfidf_svd'] = {\n",
    "            'train': vectorizers['toktok']['tfidf_svd'].transform(data['toktok']['tfidf']['train']),\n",
    "            'valid': vectorizers['toktok']['tfidf_svd'].transform(data['toktok']['tfidf']['valid']),\n",
    "            'test': vectorizers['toktok']['tfidf_svd'].transform(data['toktok']['tfidf']['test']),\n",
    "        }\n",
    "\n",
    "        # Save transformed data\n",
    "        if save_data:\n",
    "            with open(file_data, 'wb') as f:\n",
    "                pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Print shapes\n",
    "    print('Full size data shapes:')\n",
    "    print('BOW:        ',\n",
    "          data['toktok']['bow']['train'].shape,\n",
    "          data['toktok']['bow']['valid'].shape,\n",
    "          data['toktok']['bow']['test'].shape)\n",
    "    print('Hashing BOW:',\n",
    "          data['toktok']['hbow']['train'].shape,\n",
    "          data['toktok']['hbow']['valid'].shape,\n",
    "          data['toktok']['hbow']['test'].shape)\n",
    "    print('TF-IDF:     ',\n",
    "          data['toktok']['tfidf']['train'].shape,\n",
    "          data['toktok']['tfidf']['valid'].shape,\n",
    "          data['toktok']['tfidf']['test'].shape)\n",
    "    print()\n",
    "    print('SVD-truncated data shapes:')\n",
    "    print('BOW:        ',\n",
    "          data['toktok']['bow_svd']['train'].shape,\n",
    "          data['toktok']['bow_svd']['valid'].shape,\n",
    "          data['toktok']['bow_svd']['test'].shape)\n",
    "    print('Hashing BOW:',\n",
    "          data['toktok']['hbow_svd']['train'].shape,\n",
    "          data['toktok']['hbow_svd']['valid'].shape,\n",
    "          data['toktok']['hbow_svd']['test'].shape)\n",
    "    print('TF-IDF:     ',\n",
    "          data['toktok']['tfidf_svd']['train'].shape,\n",
    "          data['toktok']['tfidf_svd']['valid'].shape,\n",
    "          data['toktok']['tfidf_svd']['test'].shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5a25c-6f83-44a3-ae64-45d4fa21f445",
   "metadata": {},
   "source": [
    "Form datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb61acdd-dda5-4f45-b0a7-516a24ce2626",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if isinstance(self.x[i], np.ndarray):\n",
    "            x = self.x[i]\n",
    "        else:\n",
    "            x = self.x[i].todense()\n",
    "        return x, self.y[i]\n",
    "\n",
    "\n",
    "datasets = {'toktok': {\n",
    "    'bow': {\n",
    "        'train': Doc2VecDataset(data['toktok']['bow']['train'], dataset['train']['label']),\n",
    "        'valid': Doc2VecDataset(data['toktok']['bow']['valid'], dataset['validation']['label']),\n",
    "        'test': Doc2VecDataset(data['toktok']['bow']['test'], dataset['test']['label']),\n",
    "    },\n",
    "    'hbow': {\n",
    "        'train': Doc2VecDataset(data['toktok']['hbow']['train'], dataset['train']['label']),\n",
    "        'valid': Doc2VecDataset(data['toktok']['hbow']['valid'], dataset['validation']['label']),\n",
    "        'test': Doc2VecDataset(data['toktok']['hbow']['test'], dataset['test']['label']),\n",
    "    },\n",
    "    'tfidf': {\n",
    "        'train': Doc2VecDataset(data['toktok']['tfidf']['train'], dataset['train']['label']),\n",
    "        'valid': Doc2VecDataset(data['toktok']['tfidf']['valid'], dataset['validation']['label']),\n",
    "        'test': Doc2VecDataset(data['toktok']['tfidf']['test'], dataset['test']['label']),\n",
    "    },\n",
    "    'bow_svd': {\n",
    "        'train': Doc2VecDataset(data['toktok']['bow_svd']['train'], dataset['train']['label']),\n",
    "        'valid': Doc2VecDataset(data['toktok']['bow_svd']['valid'], dataset['validation']['label']),\n",
    "        'test': Doc2VecDataset(data['toktok']['bow_svd']['test'], dataset['test']['label']),\n",
    "    },\n",
    "    'hbow_svd': {\n",
    "        'train': Doc2VecDataset(data['toktok']['hbow_svd']['train'], dataset['train']['label']),\n",
    "        'valid': Doc2VecDataset(data['toktok']['hbow_svd']['valid'], dataset['validation']['label']),\n",
    "        'test': Doc2VecDataset(data['toktok']['hbow_svd']['test'], dataset['test']['label']),\n",
    "    },\n",
    "    'tfidf_svd': {\n",
    "        'train': Doc2VecDataset(data['toktok']['tfidf_svd']['train'], dataset['train']['label']),\n",
    "        'valid': Doc2VecDataset(data['toktok']['tfidf_svd']['valid'], dataset['validation']['label']),\n",
    "        'test': Doc2VecDataset(data['toktok']['tfidf_svd']['test'], dataset['test']['label']),\n",
    "    },\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe78e3-00ba-443a-84fc-b5e578ad7285",
   "metadata": {},
   "source": [
    "Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3c59491-350b-4cb4-8fe6-b73845eabcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples:   45615\n",
      "Validation examples: 2000\n",
      "Test examples:       12284\n",
      "\n",
      "Training batches:   713\n",
      "Validation batches: 32\n",
      "Test batches:       192\n",
      "\n",
      "Shape of X: torch.Size([64, 1, 50000]) torch.int64\n",
      "Shape of y: torch.Size([64]) torch.int64\n",
      "\n",
      "Shape of X: torch.Size([64, 100]) torch.float64\n",
      "Shape of y: torch.Size([64]) torch.int64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1555e-01,  1.6407e-01, -2.7625e-01, -2.4199e-02,  1.1401e-01,\n",
       "         3.1504e-02,  1.1226e+00, -2.4902e-01, -5.6359e-01,  3.2294e-01,\n",
       "         6.2872e-02,  2.8139e-01, -3.3960e-01,  3.8802e-02, -5.7210e-02,\n",
       "        -4.7027e-03,  3.7865e-02, -1.0003e-01,  4.3402e-02,  2.9222e-03,\n",
       "        -6.7827e-02,  1.5661e-03,  2.0842e-02,  1.6896e-02, -8.8386e-02,\n",
       "        -3.0166e-02, -5.1601e-02, -2.9432e-02, -1.3618e-03,  5.0311e-03,\n",
       "         1.8233e-03, -5.8659e-02, -3.2248e-02,  9.3290e-03,  1.9913e-02,\n",
       "        -2.9928e-02,  1.3025e-02, -3.0176e-02,  9.9205e-03,  7.6976e-03,\n",
       "        -4.1902e-02, -5.3152e-02,  1.2157e-02, -2.9063e-02,  4.9892e-02,\n",
       "         4.1866e-02, -4.7178e-02,  3.7598e-02, -4.4794e-02, -6.9791e-03,\n",
       "        -1.1249e-02,  5.5423e-02,  4.6279e-02,  1.0261e-02, -6.5526e-02,\n",
       "        -1.5512e-02, -2.3795e-02, -2.9470e-02, -1.9551e-02, -5.3367e-02,\n",
       "         4.8417e-02, -1.7730e-02,  1.0392e-01,  8.4523e-05,  7.8961e-02,\n",
       "         1.1607e-01, -8.6686e-02, -6.0226e-02, -7.4946e-02, -2.2558e-01,\n",
       "         1.4537e-01, -1.3754e-01, -1.4769e-01,  7.3627e-02, -1.5033e-01,\n",
       "         9.3529e-02,  4.4211e-02,  3.3540e-02,  1.2996e-01, -9.3171e-02,\n",
       "         8.0742e-02,  2.6368e-02, -1.9044e-01,  1.1518e-01, -2.2165e-02,\n",
       "         3.8818e-01,  1.8287e-01,  2.1045e-01,  4.4327e-01, -3.0083e-01,\n",
       "        -1.1786e-01,  8.5990e-02, -8.3996e-02, -1.0017e-01,  6.7826e-01,\n",
       "         4.5220e-01,  3.8801e-01, -6.2181e-02, -2.3829e-02,  1.8178e-01],\n",
       "       dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloaders = {'toktok': {\n",
    "    'bow': {\n",
    "        'train': DataLoader(datasets['toktok']['bow']['train'], batch_size=BATCH_SIZE, shuffle=True),\n",
    "        'valid': DataLoader(datasets['toktok']['bow']['valid'], batch_size=BATCH_SIZE),\n",
    "        'test': DataLoader(datasets['toktok']['bow']['test'], batch_size=BATCH_SIZE),\n",
    "    },\n",
    "    'hbow': {\n",
    "        'train': DataLoader(datasets['toktok']['hbow']['train'], batch_size=BATCH_SIZE, shuffle=True),\n",
    "        'valid': DataLoader(datasets['toktok']['hbow']['valid'], batch_size=BATCH_SIZE),\n",
    "        'test': DataLoader(datasets['toktok']['hbow']['test'], batch_size=BATCH_SIZE),\n",
    "    },\n",
    "    'tfidf': {\n",
    "        'train': DataLoader(datasets['toktok']['tfidf']['train'], batch_size=BATCH_SIZE, shuffle=True),\n",
    "        'valid': DataLoader(datasets['toktok']['tfidf']['valid'], batch_size=BATCH_SIZE),\n",
    "        'test': DataLoader(datasets['toktok']['tfidf']['test'], batch_size=BATCH_SIZE),\n",
    "    },\n",
    "    'bow_svd': {\n",
    "        'train': DataLoader(datasets['toktok']['bow_svd']['train'], batch_size=BATCH_SIZE, shuffle=True),\n",
    "        'valid': DataLoader(datasets['toktok']['bow_svd']['valid'], batch_size=BATCH_SIZE),\n",
    "        'test': DataLoader(datasets['toktok']['bow_svd']['test'], batch_size=BATCH_SIZE),\n",
    "    },\n",
    "    'hbow_svd': {\n",
    "        'train': DataLoader(datasets['toktok']['hbow_svd']['train'], batch_size=BATCH_SIZE, shuffle=True),\n",
    "        'valid': DataLoader(datasets['toktok']['hbow_svd']['valid'], batch_size=BATCH_SIZE),\n",
    "        'test': DataLoader(datasets['toktok']['hbow_svd']['test'], batch_size=BATCH_SIZE),\n",
    "    },\n",
    "    'tfidf_svd': {\n",
    "        'train': DataLoader(datasets['toktok']['tfidf_svd']['train'], batch_size=BATCH_SIZE, shuffle=True),\n",
    "        'valid': DataLoader(datasets['toktok']['tfidf_svd']['valid'], batch_size=BATCH_SIZE),\n",
    "        'test': DataLoader(datasets['toktok']['tfidf_svd']['test'], batch_size=BATCH_SIZE),\n",
    "    },\n",
    "}}\n",
    "\n",
    "# Show shapes and types\n",
    "print('Training examples:  ', len(dataloaders['toktok']['bow']['train'].dataset))\n",
    "print('Validation examples:', len(dataloaders['toktok']['bow']['valid'].dataset))\n",
    "print('Test examples:      ', len(dataloaders['toktok']['bow']['test'].dataset))\n",
    "print()\n",
    "print('Training batches:  ', len(dataloaders['toktok']['bow']['train']))\n",
    "print('Validation batches:', len(dataloaders['toktok']['bow']['valid']))\n",
    "print('Test batches:      ', len(dataloaders['toktok']['bow']['test']))\n",
    "print()\n",
    "X, y = next(iter(dataloaders['toktok']['bow']['train']))\n",
    "print(f'Shape of X: {X.shape} {X.dtype}')\n",
    "print(f'Shape of y: {y.shape} {y.dtype}')\n",
    "print()\n",
    "X, y = next(iter(dataloaders['toktok']['bow_svd']['train']))\n",
    "print(f'Shape of X: {X.shape} {X.dtype}')\n",
    "print(f'Shape of y: {y.shape} {y.dtype}')\n",
    "print()\n",
    "display(X[0])\n",
    "display(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51485a-e469-4dba-8e1f-e49f62abde40",
   "metadata": {},
   "source": [
    "## Token vectorizers\n",
    "### Word2Vec\n",
    "#### Train from the ground up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "beb38de3-5e15-401c-8e73-30b1e2662e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary shape:\n",
      "(10611, 100)\n",
      "\n",
      "Most frequent words:\n",
      "\"\n",
      "@user\n",
      "'\n",
      ",\n",
      "!\n",
      ".\n",
      ":\n",
      "...\n",
      "?\n",
      "may\n",
      "tomorrow\n",
      "go\n",
      ")\n",
      "day\n",
      "-\n",
      "get\n",
      "see\n",
      "like\n",
      "(\n",
      ";\n",
      "\n",
      "CPU times: user 1.8 ms, sys: 1.89 ms, total: 3.69 ms\n",
      "Wall time: 4.27 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vector_size = 100\n",
    "file = WORKING_PATH + 'word2vec_' + str(vector_size) + '.gensim'\n",
    "saving = True\n",
    "\n",
    "# Load model if it already exists\n",
    "if os.path.isfile(file):\n",
    "    word2vec = gensim.models.KeyedVectors.load(file, mmap='r')\n",
    "\n",
    "else:\n",
    "    # Initialize tokenizer wiht corpus in it\n",
    "    tokenizer = sentiment_utils.Tokenizer(dataset['train']['text']\n",
    "                                          + dataset['test']['text']\n",
    "                                          + dataset['validation']['text'])\n",
    "\n",
    "    # Train the model\n",
    "    word2vec = gensim.models.Word2Vec(\n",
    "        sentences=tokenizer, vector_size=vector_size, window=5, min_count=5, sg=1, hs=0, negative=5,\n",
    "        workers=7, epochs=5, seed=RANDOM_STATE,\n",
    "    )\n",
    "    \n",
    "    # Use the word vectors only\n",
    "    word2vec = word2vec.wv\n",
    "\n",
    "    # Save the model word vectors\n",
    "    if saving:\n",
    "        word2vec.save(file)\n",
    "\n",
    "# Print vocabulary shape\n",
    "print('Vocabulary shape:')\n",
    "print((len(word2vec.index_to_key), vector_size))\n",
    "print()\n",
    "\n",
    "# Print most frequent words\n",
    "print('Most frequent words:')\n",
    "for word in word2vec.index_to_key[:20]:\n",
    "    print(word)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20dec7-403d-4e23-b2eb-2ebda1ca5984",
   "metadata": {},
   "source": [
    "Show examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b4a724d-eae0-4972-9e91-362ce7271af4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A vector example:\n",
      "[-6.7988336e-03 -7.7007711e-03 -6.7419447e-03  7.7721477e-03\n",
      " -9.1446610e-03 -6.6873073e-03 -6.6153635e-03 -2.2669220e-03\n",
      "  5.0509833e-03  5.8403742e-03  6.4396439e-03  8.6656129e-03\n",
      " -8.7526087e-03 -9.2006801e-04 -1.6529012e-03 -6.5322830e-03\n",
      " -3.4659612e-03 -1.9954813e-03  8.2546510e-03  1.9973540e-03\n",
      " -9.0243109e-03  4.0886807e-03 -5.3359149e-04 -2.5054060e-03\n",
      " -6.9734524e-03 -4.2239283e-03 -1.2363232e-03  1.5906275e-03\n",
      "  1.5835894e-03  6.6484306e-03 -1.8646896e-03  9.8702870e-03\n",
      "  9.3534179e-03 -8.1601581e-03 -3.8998926e-03 -6.2233713e-03\n",
      " -3.3651828e-04  2.3092914e-03 -2.8936565e-03 -3.0549956e-03\n",
      "  3.3477665e-04 -2.8081452e-03 -7.9259863e-03 -8.3585903e-03\n",
      "  6.7217945e-04  9.0850675e-03 -8.8485815e-03 -3.2784594e-03\n",
      " -1.6568815e-03  7.9573207e-03  2.2853673e-03 -1.6162921e-03\n",
      " -7.9821423e-03  3.6615168e-03 -2.7477740e-06  2.6824963e-03\n",
      " -9.2297187e-03 -8.0831572e-03  2.4737692e-03  4.3313741e-03\n",
      " -6.3958620e-03 -1.2299264e-03  1.1683321e-03  9.0518082e-03\n",
      "  3.1548619e-04  3.0346382e-03  4.2372416e-03  6.7232894e-03\n",
      " -3.3694482e-03  1.9274366e-03  5.5399071e-03  2.0930672e-03\n",
      " -1.5624798e-03  5.8541680e-03 -6.7653833e-03  5.1936209e-03\n",
      "  2.0783448e-03  2.3969626e-03 -1.6580462e-04  6.9747807e-04\n",
      " -5.3472710e-03  1.8660760e-03 -6.5726280e-04  5.8715190e-03\n",
      "  7.2001112e-03  1.9464636e-03 -7.0486926e-03 -2.4714721e-03\n",
      "  8.2157822e-03  4.9938383e-03 -9.0842601e-03 -7.7735009e-03\n",
      "  2.8241635e-03 -4.6629561e-03 -2.8908777e-03  1.8842184e-03\n",
      "  5.3190198e-03  5.8819582e-03 -2.7071154e-03  4.0055071e-03]\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Print the vector example\n",
    "print('A vector example:')\n",
    "print(word2vec['@user'])\n",
    "print(word2vec['@user'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee6bb43-a899-4c87-bdbb-13b4cc7c53bc",
   "metadata": {},
   "source": [
    "### fastText\n",
    "#### Train from the ground up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7469688a-3d07-4084-a22f-cd8123778a88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary shape:\n",
      "(10611, 100)\n",
      "\n",
      "Most frequent words:\n",
      "\"\n",
      "@user\n",
      "'\n",
      ",\n",
      "!\n",
      ".\n",
      ":\n",
      "...\n",
      "?\n",
      "may\n",
      "tomorrow\n",
      "go\n",
      ")\n",
      "day\n",
      "-\n",
      "get\n",
      "see\n",
      "like\n",
      "(\n",
      ";\n",
      "\n",
      "CPU times: user 386 ms, sys: 311 ms, total: 697 ms\n",
      "Wall time: 2.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vector_size = 100\n",
    "file = WORKING_PATH + 'fasttext_' + str(vector_size) + '.gensim'\n",
    "saving = True\n",
    "\n",
    "# Load model if it already exists\n",
    "if os.path.isfile(file):\n",
    "    fasttext = gensim.models.fasttext.FastTextKeyedVectors.load(file, mmap='r')\n",
    "\n",
    "else:\n",
    "    # Initialize tokenizer wiht corpus in it\n",
    "    tokenizer = sentiment_utils.Tokenizer(dataset['train']['text']\n",
    "                                          + dataset['test']['text']\n",
    "                                          + dataset['validation']['text'])\n",
    "\n",
    "    # Train the model\n",
    "    fasttext = gensim.models.FastText(\n",
    "        sentences=tokenizer, vector_size=vector_size, window=5, min_count=5, sg=1, hs=0, negative=5,\n",
    "        workers=7, epochs=5, seed=RANDOM_STATE,\n",
    "    )\n",
    "    \n",
    "    # Use the word vectors only\n",
    "    fasttext = fasttext.wv\n",
    "\n",
    "    # Save the model word vectors\n",
    "    if saving:\n",
    "        fasttext.save(file)\n",
    "\n",
    "# Print vocabulary shape\n",
    "print('Vocabulary shape:')\n",
    "print((len(fasttext.index_to_key), vector_size))\n",
    "print()\n",
    "\n",
    "# Print most frequent words\n",
    "print('Most frequent words:')\n",
    "for word in fasttext.index_to_key[:20]:\n",
    "    print(word)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed2c73-7434-47f7-8091-c71a15bec868",
   "metadata": {},
   "source": [
    "Show examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09f0bd21-0d25-44c9-9f5c-54adb58eed47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A vector example:\n",
      "[-4.87358047e-04  4.51807398e-04  8.77588638e-04 -3.32686584e-04\n",
      " -1.02142920e-03 -5.65615657e-04 -1.85830169e-03 -1.64301752e-03\n",
      " -3.25506413e-03  4.57338197e-03  1.27422193e-03  3.48202884e-03\n",
      "  1.94513152e-04  1.38777623e-05  2.34347157e-04  1.86236299e-04\n",
      "  1.52681104e-03  1.45538780e-03 -2.91749515e-04 -8.49718112e-04\n",
      " -1.08485331e-03  7.60952767e-04  2.18979130e-03  1.52153475e-03\n",
      " -8.63333640e-04 -1.12725690e-03 -4.13653994e-04 -9.69837129e-04\n",
      " -3.50951846e-03  2.00337311e-03  2.72217090e-03  9.62128979e-04\n",
      " -9.16386663e-04 -2.23301514e-03 -9.92241781e-04 -4.96376480e-04\n",
      " -2.19261716e-03  4.06168081e-04 -2.76850234e-03 -2.22463836e-03\n",
      "  1.32712605e-03 -9.71838774e-04 -3.71666916e-04 -3.45141103e-04\n",
      "  2.19832268e-03 -4.77933296e-04 -1.50938821e-03 -8.66060960e-04\n",
      "  1.31140207e-03 -1.85101863e-03  1.01888634e-03  2.18549496e-04\n",
      " -1.96930929e-03 -6.17635378e-04  2.47252802e-03 -8.30107136e-04\n",
      "  1.03148588e-04  3.78433871e-03 -1.67751324e-03  8.75202590e-04\n",
      " -7.45509460e-04 -7.74183136e-04 -1.29649055e-03 -3.31599877e-04\n",
      " -3.02510476e-03  2.44991155e-03 -1.57515251e-03  1.12388586e-03\n",
      " -2.91600311e-03 -4.86238365e-04  1.43003138e-03 -7.21334654e-04\n",
      "  2.63229012e-03  3.56963661e-04  1.17806520e-03 -8.68825533e-04\n",
      " -6.35157165e-04  8.25674972e-04  1.32847368e-03  2.76971678e-03\n",
      " -6.66425563e-04  4.56986832e-04  2.44077365e-03 -1.48916186e-03\n",
      "  2.13712151e-03 -8.01523274e-04 -8.83655041e-04 -4.45649057e-04\n",
      " -1.41387770e-03  1.21046486e-03 -1.74477522e-03  3.50933144e-04\n",
      " -9.27234869e-05 -1.16335781e-04 -2.03814032e-03  1.53253477e-05\n",
      "  8.33417071e-05  2.39008456e-03 -1.16614625e-04  1.16813590e-03]\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Print the vector example\n",
    "print('A vector example:')\n",
    "print(fasttext['@user'])\n",
    "print(fasttext['@user'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398f7c57-6a5b-45cb-acb7-2aca597d738e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Computational graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b31da21-e7a5-4f43-bc60-e65c5bc73086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of models\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f84d80f9-8a68-4b06-938f-73b835a5509a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For \"toktok_bow_logreg\" model:\n",
      "- all params:       [150000, 3]   total: 150003\n",
      "- trainable params: [150000, 3]   total: 150003\n",
      "- LogRegModel(\n",
      "  (linear): Linear(in_features=50000, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "For \"toktok_bow_svd_logreg\" model:\n",
      "- all params:       [300, 3]   total: 303\n",
      "- trainable params: [300, 3]   total: 303\n",
      "- LogRegModel(\n",
      "  (linear): Linear(in_features=100, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LogRegModel(nn.Module):\n",
    "    \"\"\"Logistic Regression model\"\"\"\n",
    "    def __init__(self, n_neurons):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(n_neurons, CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear(x)\n",
    "        return logits\n",
    "\n",
    "models['toktok_bow_logreg'] = LogRegModel(VOCAB_SIZE)\n",
    "models['toktok_hbow_logreg'] = LogRegModel(VOCAB_SIZE)\n",
    "models['toktok_tfidf_logreg'] = LogRegModel(VOCAB_SIZE)\n",
    "models['toktok_bow_svd_logreg'] = LogRegModel(SVD_SIZE)\n",
    "models['toktok_hbow_svd_logreg'] = LogRegModel(SVD_SIZE)\n",
    "models['toktok_tfidf_svd_logreg'] = LogRegModel(SVD_SIZE)\n",
    "\n",
    "# Print model info\n",
    "model = 'toktok_bow_logreg'\n",
    "params = [p.numel() for p in models[model].parameters()]\n",
    "print(f'For \"{model}\" model:')\n",
    "print('- all params:      ', params, '  total:', sum(params))\n",
    "params = [p.numel() for p in models[model].parameters() if p.requires_grad]\n",
    "print('- trainable params:', params, '  total:', sum(params))\n",
    "print('-', models[model])\n",
    "print()\n",
    "model = 'toktok_bow_svd_logreg'\n",
    "params = [p.numel() for p in models[model].parameters()]\n",
    "print(f'For \"{model}\" model:')\n",
    "print('- all params:      ', params, '  total:', sum(params))\n",
    "params = [p.numel() for p in models[model].parameters() if p.requires_grad]\n",
    "print('- trainable params:', params, '  total:', sum(params))\n",
    "print('-', models[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40b4423d-17c0-4a89-8dfb-40723ef26f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For \"toktok_bow_dense2\" model:\n",
      "- all params:       [5000000, 100, 300, 3]   total: 5000403\n",
      "- trainable params: [5000000, 100, 300, 3]   total: 5000403\n",
      "- Dense2Model(\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=50000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "For \"toktok_bow_svd_dense2\" model:\n",
      "- all params:       [10000, 100, 300, 3]   total: 10403\n",
      "- trainable params: [10000, 100, 300, 3]   total: 10403\n",
      "- Dense2Model(\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Dense2Model(nn.Module):\n",
    "    \"\"\"Dense model with 2 fully connected layers\"\"\"\n",
    "    def __init__(self, n_neurons):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(n_neurons[0], n_neurons[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_neurons[1], CLASSES),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "\n",
    "models['toktok_bow_dense2'] = Dense2Model((VOCAB_SIZE, SVD_SIZE))\n",
    "models['toktok_hbow_dense2'] = Dense2Model((VOCAB_SIZE, SVD_SIZE))\n",
    "models['toktok_tfidf_dense2'] = Dense2Model((VOCAB_SIZE, SVD_SIZE))\n",
    "models['toktok_bow_svd_dense2'] = Dense2Model((SVD_SIZE, SVD_SIZE))\n",
    "models['toktok_hbow_svd_dense2'] = Dense2Model((SVD_SIZE, SVD_SIZE))\n",
    "models['toktok_tfidf_svd_dense2'] = Dense2Model((SVD_SIZE, SVD_SIZE))\n",
    "\n",
    "# Print model info\n",
    "model = 'toktok_bow_dense2'\n",
    "params = [p.numel() for p in models[model].parameters()]\n",
    "print(f'For \"{model}\" model:')\n",
    "print('- all params:      ', params, '  total:', sum(params))\n",
    "params = [p.numel() for p in models[model].parameters() if p.requires_grad]\n",
    "print('- trainable params:', params, '  total:', sum(params))\n",
    "print('-', models[model])\n",
    "print()\n",
    "model = 'toktok_bow_svd_dense2'\n",
    "params = [p.numel() for p in models[model].parameters()]\n",
    "print(f'For \"{model}\" model:')\n",
    "print('- all params:      ', params, '  total:', sum(params))\n",
    "params = [p.numel() for p in models[model].parameters() if p.requires_grad]\n",
    "print('- trainable params:', params, '  total:', sum(params))\n",
    "print('-', models[model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3850577d-d08e-480f-aca1-d5bee071de72",
   "metadata": {},
   "source": [
    "## Range test for learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f410a07b-66a9-4a78-b8cc-31dddffda1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toktok_bow_logreg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213893ebc24e47feab02151ded39e755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Range test for LR\u001b[39;00m\n\u001b[1;32m     16\u001b[0m lr_finder \u001b[38;5;241m=\u001b[39m LRFinder(model, optimizer, loss_fn)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mlr_finder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrange_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtoktok\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtoktok\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mend_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_lr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mnum_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Inspect the loss-LR graph\u001b[39;00m\n\u001b[1;32m     23\u001b[0m lr_finder\u001b[38;5;241m.\u001b[39mplot()\n",
      "File \u001b[0;32m~/Documents/GitHub/ml-service/venv/lib/python3.11/site-packages/torch_lr_finder/lr_finder.py:317\u001b[0m, in \u001b[0;36mLRFinder.range_test\u001b[0;34m(self, train_loader, val_loader, start_lr, end_lr, num_iter, step_mode, smooth_f, diverge_th, accumulation_steps, non_blocking_transfer)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    310\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`val_loader` has unsupported type: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected types are `torch.utils.data.DataLoader`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor child of `ValDataLoaderIter`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(val_loader))\n\u001b[1;32m    313\u001b[0m         )\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_iter)):\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# Train on batch and retrieve loss\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking_transfer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking_transfer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_loader:\n\u001b[1;32m    323\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate(\n\u001b[1;32m    324\u001b[0m             val_iter, non_blocking_transfer\u001b[38;5;241m=\u001b[39mnon_blocking_transfer\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/GitHub/ml-service/venv/lib/python3.11/site-packages/torch_lr_finder/lr_finder.py:377\u001b[0m, in \u001b[0;36mLRFinder._train_batch\u001b[0;34m(self, train_iter, accumulation_steps, non_blocking_transfer)\u001b[0m\n\u001b[1;32m    372\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_move_to_device(\n\u001b[1;32m    373\u001b[0m     inputs, labels, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking_transfer\n\u001b[1;32m    374\u001b[0m )\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, labels)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# Loss should be averaged in each step\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/ml-service/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[42], line 8\u001b[0m, in \u001b[0;36mLogRegModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 8\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/Documents/GitHub/ml-service/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/ml-service/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "if DO_LR_RANGE_TEST:\n",
    "    start_lr = 1e-4\n",
    "    end_lr = 1e1\n",
    "    num_iter = 50\n",
    "\n",
    "    # Loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(name)\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=start_lr)\n",
    "\n",
    "        # Range test for LR\n",
    "        lr_finder = LRFinder(model, optimizer, loss_fn)\n",
    "        lr_finder.range_test(train_loader=dataloaders['toktok']['bow']['train'],\n",
    "                             val_loader=dataloaders['toktok']['bow']['valid'],\n",
    "                             end_lr=end_lr,\n",
    "                             num_iter=num_iter)\n",
    "        \n",
    "        # Inspect the loss-LR graph\n",
    "        lr_finder.plot()\n",
    "        \n",
    "        # Reset the model and optimizer to their initial state\n",
    "        lr_finder.reset()\n",
    "else:\n",
    "    lrs = {\n",
    "        'logreg': 1e-1,\n",
    "        'dense3': 6e-1,\n",
    "        'conv3': 1e-1,\n",
    "        'conv5': 4e-1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a46bd77-841a-4888-a9e9-54791767c181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
