{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76dd1590-1dd7-4380-93b7-4491789eb84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport sentiment_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff51efb6-9982-433e-b9b7-73a3ad88d354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python:          3.10.11 (main, Apr  7 2023, 07:24:53) [Clang 14.0.0 (clang-1400.0.29.202)]\n",
      "scikit-learn:    1.2.2\n",
      "Gensim:          4.3.1\n",
      "PyTorch:         1.13.1\n",
      "Transformers:    4.28.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import datasets\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, HashingVectorizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print('python:'.ljust(16), sys.version.split('\\n')[0])\n",
    "print('scikit-learn:'.ljust(16), sklearn.__version__)\n",
    "print('Gensim:'.ljust(16), gensim.__version__)\n",
    "print('PyTorch:'.ljust(16), torch.__version__)\n",
    "print('Transformers:'.ljust(16), transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d3a80-84d9-4a1a-996a-bc30784d09db",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5038b4-d83c-4bde-a230-0ca7538d5b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {DEVICE} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d1d08d-6b4f-4979-8ff5-b397c84c0383",
   "metadata": {},
   "source": [
    "# Hyperparameters & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6420d850-2109-4edc-937f-246d5d0312cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# BATCH_SIZE = 64\n",
    "# EPOCHS = 15  # select from: 2**n - 1 = [1, 3, 7, 15, ...]\n",
    "# SCHEDULER_GAMMA = 0.7\n",
    "\n",
    "# Constants\n",
    "WORKING_PATH = './sentiment-data/'\n",
    "MODEL_PATH = '../app/models/'\n",
    "DATASET_NAME = 'tweet_eval'\n",
    "DATASET_CONF = 'sentiment'\n",
    "CLASSES = 3\n",
    "HUGGINGFACE_MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "\n",
    "# Actions\n",
    "# DO_LR_RANGE_TEST=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59c5513-1785-4de5-a58b-797d6ddd243e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a433eb7-c8d0-44bf-a7d1-867166772870",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2147483647\n",
    "# random.seed(RANDOM_STATE)\n",
    "# np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed_all(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a62e08-a57d-46f5-8d21-74df9a240575",
   "metadata": {},
   "source": [
    "# Load & show data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbc60f77-c4fe-459c-9a84-e2c674833aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tweet_eval (/Users/admin/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e08f06bf4e54432b305ddee4ecdacd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 45615\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12284\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(DATASET_NAME, DATASET_CONF)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a608d6b-b412-4319-8576-02575aa5d512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"',\n",
       "  '\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"',\n",
       "  'Sorry bout the stream last night I crashed out but will be on tonight for sure. Then back to Minecraft in pc tomorrow night.',\n",
       "  \"Chase Headley's RBI double in the 8th inning off David Price snapped a Yankees streak of 33 consecutive scoreless innings against Blue Jays\",\n",
       "  '@user Alciato: Bee will invest 150 million in January, another 200 in the Summer and plans to bring Messi by 2017\"'],\n",
       " 'label': [2, 1, 1, 1, 2]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96a266-7271-4ae2-816c-a5bdf86037b3",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "## TokTokTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3040eda8-5742-4c0d-b412-343ad7dd3fee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"\n",
      "\" qt @user origin draft 7th book , remu lupin surviv battl hogwarts. #happybirthdayremuslupin \"\n",
      "\n",
      "\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"\n",
      "\" ben smith / smith ( concuss ) remain lineup thursday , curti #nhl #sj \"\n",
      "\n",
      "Sorry bout the stream last night I crashed out but will be on tonight for sure. Then back to Minecraft in pc tomorrow night.\n",
      "sorri bout stream last night crash tonight sure. back minecraft pc tomorrow night .\n",
      "\n",
      "Chase Headley's RBI double in the 8th inning off David Price snapped a Yankees streak of 33 consecutive scoreless innings against Blue Jays\n",
      "chase headley ' rbi doubl 8th inning david price snap yanke streak 33 consecut scoreless inning blue jay\n",
      "\n",
      "@user Alciato: Bee will invest 150 million in January, another 200 in the Summer and plans to bring Messi by 2017\"\n",
      "@user alciato : bee invest 150 million januari , anoth 200 summer plan bring messi 2017 \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = sentiment_utils.Tokenizer()\n",
    "\n",
    "# Print tokenization examples\n",
    "for text in dataset['train']['text'][:5]:\n",
    "    print(text)\n",
    "    print(tokenizer(text, return_str=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1283f037-250c-4c24-b8e1-4a6fd5a4bfed",
   "metadata": {},
   "source": [
    "## RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed5173ee-4af2-4dbb-87b8-e99ff4fd413e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"\n",
      "tensor([[    0,   113,  1864,   565,   787, 12105,    96,     5,  1461,  2479,\n",
      "             9,     5,   262,   212,  1040,     6,  8022,   687, 26110,   179,\n",
      "          5601,     5,  9846,     9, 42210,     4,   849, 21136, 44728,  1208,\n",
      "         31157,   687,   574,   658,   179,   113,    22,  1864,   565,   787,\n",
      "         12105,    96,     5,  1461,  2479,     9,     5,   262,   212,  1040,\n",
      "             6,  8022,   687, 26110,   179,  5601,     5,  9846,     9, 42210,\n",
      "             4,   849, 21136, 44728,  1208, 31157,   687,   574,   658,   179,\n",
      "           113,     2]])\n",
      "\n",
      "\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"\n",
      "tensor([[    0,   113, 17521,  1259,  1589,  1259,    36,  3865, 33825,    43,\n",
      "          1189,    66,     9,     5,  4451,   296,     6, 11292,   849,   487,\n",
      "          8064,   849,   104,   863,   113,    22, 17521,  1259,  1589,  1259,\n",
      "            36,  3865, 33825,    43,  1189,    66,     9,     5,  4451,   296,\n",
      "             6, 11292,   849,   487,  8064,   849,   104,   863,   113,     2]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(HUGGINGFACE_MODEL_NAME)\n",
    "\n",
    "for text in dataset['train']['text'][:2]:\n",
    "    print(text)\n",
    "    preprocessed_text = sentiment_utils.preprocess_text(text)\n",
    "    print(alv_pretrained(preprocessed_text, return_tensors='pt')['input_ids'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5994ec64-fd12-4eab-83a1-180fc2c7c8fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(alv_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "980647cb-223f-4652-b443-7ff76cb51afc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.98 s, sys: 125 ms, total: 9.1 s\n",
      "Wall time: 2.13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 45615\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 12284\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# To process the whole corpus\n",
    "def preprocess_function(examples):\n",
    "    preprocessed_text = sentiment_utils.preprocess_text(examples['text'])\n",
    "    return alv_pretrained(preprocessed_text)\n",
    "\n",
    "dataset_alv = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "dataset_alv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29efae-6858-4dd6-98d4-7f0824f53130",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "## Document vectorizers\n",
    "### BOW, TF-IDF, Hashing BOW and their SVD variants\n",
    "Fit vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a363196-375b-4a06-a9c2-c648e0ce46f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/Documents/GitHub/ml-service/venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance for SVD:\n",
      "BOW:         51.53 %\n",
      "TF-IDF:      16.51 %\n",
      "Hashing BOW: 41.75 %\n",
      "\n",
      "CPU times: user 56.8 s, sys: 17.6 s, total: 1min 14s\n",
      "Wall time: 22.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_features = 50000\n",
    "svd_components = 100\n",
    "save_vectorizers = True\n",
    "file_vectorizers = WORKING_PATH + 'bow_tfidf_vectorizers_' + str(n_features) + '_' + str(svd_components) + '.pickle'\n",
    "\n",
    "# Load vectorizer if it already exists\n",
    "if os.path.isfile(file_vectorizers):\n",
    "    with open(file_vectorizers, 'rb') as f:\n",
    "        vectorizers = pickle.load(f)\n",
    "        \n",
    "    (\n",
    "        bow_vectorizer, tfidf_vectorizer, hashing_vectorizer,\n",
    "        svd_bow_vectorizer, svd_tfidf_vectorizer, svd_hashing_vectorizer,\n",
    "    ) = vectorizers\n",
    "\n",
    "else:\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = sentiment_utils.Tokenizer()\n",
    "\n",
    "    # Initialize vectorizers\n",
    "    bow_vectorizer = CountVectorizer(lowercase=False,\n",
    "                                     tokenizer=tokenizer,\n",
    "                                     max_features=n_features)\n",
    "    tfidf_vectorizer = TfidfTransformer()\n",
    "    hashing_vectorizer = HashingVectorizer(lowercase=False,\n",
    "                                           tokenizer=tokenizer,\n",
    "                                           n_features=n_features)\n",
    "\n",
    "    # Initialize SVD-truncated vectorizers\n",
    "    svd_bow_vectorizer = TruncatedSVD(n_components=svd_components)\n",
    "    svd_tfidf_vectorizer = TruncatedSVD(n_components=svd_components)\n",
    "    svd_hashing_vectorizer = TruncatedSVD(n_components=svd_components)\n",
    "\n",
    "    # Fit vectorizers and transform train data\n",
    "    x_train_bow = bow_vectorizer.fit_transform(dataset['train']['text'])\n",
    "    x_train_tfidf = tfidf_vectorizer.fit_transform(x_train_bow)\n",
    "    x_train_hashing = hashing_vectorizer.fit_transform(dataset['train']['text'])\n",
    "\n",
    "    # Fit SVD-truncated vectorizers\n",
    "    svd_bow_vectorizer.fit(x_train_bow)\n",
    "    svd_tfidf_vectorizer.fit(x_train_tfidf)\n",
    "    svd_hashing_vectorizer.fit(x_train_hashing)\n",
    "    \n",
    "    # Save vectorizers\n",
    "    if save_vectorizers:\n",
    "        vectorizers = (\n",
    "            bow_vectorizer, tfidf_vectorizer, hashing_vectorizer,\n",
    "            svd_bow_vectorizer, svd_tfidf_vectorizer, svd_hashing_vectorizer,\n",
    "        )\n",
    "\n",
    "        with open(file_vectorizers, 'wb') as f:\n",
    "            pickle.dump(vectorizers, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Print SVD explained variance\n",
    "print('Explained variance for SVD:')\n",
    "print(f'BOW:         {(svd_bow_vectorizer.explained_variance_ratio_.sum() * 100):.2f} %')\n",
    "print(f'TF-IDF:      {(svd_tfidf_vectorizer.explained_variance_ratio_.sum() * 100):.2f} %')\n",
    "print(f'Hashing BOW: {(svd_hashing_vectorizer.explained_variance_ratio_.sum() * 100):.2f} %')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21281e38-f9d6-40b4-94db-fd66c1bf3165",
   "metadata": {},
   "source": [
    "Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aded4063-7e5f-4fe7-8689-54262cfc545b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full size data shapes:\n",
      "BOW:         (45615, 50000) (2000, 50000) (12284, 50000)\n",
      "TF-IDF:      (45615, 50000) (2000, 50000) (12284, 50000)\n",
      "Hashing BOW: (45615, 50000) (2000, 50000) (12284, 50000)\n",
      "\n",
      "SVD-truncated data shapes:\n",
      "BOW:         (45615, 100) (2000, 100) (12284, 100)\n",
      "TF-IDF:      (45615, 100) (2000, 100) (12284, 100)\n",
      "Hashing BOW: (45615, 100) (2000, 100) (12284, 100)\n",
      "\n",
      "CPU times: user 18.7 s, sys: 106 ms, total: 18.8 s\n",
      "Wall time: 18.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transform_data = True\n",
    "save_data = True\n",
    "file_data = WORKING_PATH + 'bow_tfidf_data_' + str(n_features) + '_' + str(svd_components) + '.pickle'\n",
    "\n",
    "if transform_data:\n",
    "    # Load transformed data if it already exists\n",
    "    if os.path.isfile(file_data):\n",
    "        with open(file_data, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        (\n",
    "            x_train_bow, x_valid_bow, x_test_bow,\n",
    "            x_train_tfidf, x_valid_tfidf, x_test_tfidf,\n",
    "            x_train_hashing, x_valid_hashing, x_test_hashing,\n",
    "            x_train_svd_bow, x_valid_svd_bow, x_test_svd_bow,\n",
    "            x_train_svd_tfidf, x_valid_svd_tfidf, x_test_svd_tfidf,\n",
    "            x_train_svd_hashing, x_valid_svd_hashing, x_test_svd_hashing,\n",
    "        ) = data\n",
    "\n",
    "    else:\n",
    "        # Fit vectorizers and transform train data\n",
    "        x_train_bow = bow_vectorizer.transform(dataset['train']['text'])\n",
    "        x_train_tfidf = tfidf_vectorizer.transform(x_train_bow)\n",
    "        x_train_hashing = hashing_vectorizer.transform(dataset['train']['text'])\n",
    "\n",
    "        # Transform train data for SVD-truncated vectorizers\n",
    "        x_train_svd_bow = svd_bow_vectorizer.transform(x_train_bow)\n",
    "        x_train_svd_tfidf = svd_tfidf_vectorizer.transform(x_train_tfidf)\n",
    "        x_train_svd_hashing = svd_hashing_vectorizer.transform(x_train_hashing)\n",
    "\n",
    "        # Transform validation and test data\n",
    "        x_valid_bow = bow_vectorizer.transform(dataset['validation']['text'])\n",
    "        x_valid_tfidf = tfidf_vectorizer.transform(x_valid_bow)\n",
    "        x_valid_hashing = hashing_vectorizer.transform(dataset['validation']['text'])\n",
    "        x_test_bow = bow_vectorizer.transform(dataset['test']['text'])\n",
    "        x_test_tfidf = tfidf_vectorizer.transform(x_test_bow)\n",
    "        x_test_hashing = hashing_vectorizer.transform(dataset['test']['text'])\n",
    "\n",
    "        # Transform validation and test data for SVD-truncated vectorizers\n",
    "        x_valid_svd_bow = svd_bow_vectorizer.transform(x_valid_bow)\n",
    "        x_valid_svd_tfidf = svd_tfidf_vectorizer.transform(x_valid_tfidf)\n",
    "        x_valid_svd_hashing = svd_hashing_vectorizer.transform(x_valid_hashing)\n",
    "        x_test_svd_bow = svd_bow_vectorizer.transform(x_test_bow)\n",
    "        x_test_svd_tfidf = svd_tfidf_vectorizer.transform(x_test_tfidf)\n",
    "        x_test_svd_hashing = svd_hashing_vectorizer.transform(x_test_hashing)\n",
    "\n",
    "        # Save transformed data\n",
    "        if save_data:\n",
    "            data = (\n",
    "                x_train_bow, x_valid_bow, x_test_bow,\n",
    "                x_train_tfidf, x_valid_tfidf, x_test_tfidf,\n",
    "                x_train_hashing, x_valid_hashing, x_test_hashing,\n",
    "                x_train_svd_bow, x_valid_svd_bow, x_test_svd_bow,\n",
    "                x_train_svd_tfidf, x_valid_svd_tfidf, x_test_svd_tfidf,\n",
    "                x_train_svd_hashing, x_valid_svd_hashing, x_test_svd_hashing,\n",
    "            )\n",
    "\n",
    "            with open(file_data, 'wb') as f:\n",
    "                pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Print shapes\n",
    "    print('Full size data shapes:')\n",
    "    print('BOW:        ', x_train_bow.shape, x_valid_bow.shape, x_test_bow.shape)\n",
    "    print('TF-IDF:     ', x_train_tfidf.shape, x_valid_tfidf.shape, x_test_tfidf.shape)\n",
    "    print('Hashing BOW:', x_train_hashing.shape, x_valid_hashing.shape, x_test_hashing.shape)\n",
    "    print()\n",
    "    print('SVD-truncated data shapes:')\n",
    "    print('BOW:        ', x_train_svd_bow.shape, x_valid_svd_bow.shape, x_test_svd_bow.shape)\n",
    "    print('TF-IDF:     ', x_train_svd_tfidf.shape, x_valid_svd_tfidf.shape, x_test_svd_tfidf.shape)\n",
    "    print('Hashing BOW:', x_train_svd_hashing.shape, x_valid_svd_hashing.shape, x_test_svd_hashing.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e4dee-b3f8-4b15-947f-58096b88b1f3",
   "metadata": {},
   "source": [
    "Show examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3252149e-5ec6-43c3-9407-036cb06ab72a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document vector examples and their shapes:\n",
      "\n",
      "BOW:\n",
      "[[0 2 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]]\n",
      "(2, 50000)\n",
      "\n",
      "TF-IDF:\n",
      "[[0.         0.1592305  0.         ... 0.         0.         0.        ]\n",
      " [0.         0.16949097 0.         ... 0.         0.         0.        ]]\n",
      "(2, 50000)\n",
      "\n",
      "Hashing BOW:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(2, 50000)\n",
      "\n",
      "SVD BOW:\n",
      "[ 2.19219037  0.02814938 -0.37149003  0.46869895  0.77413124 -0.27684479]\n",
      "(2, 100)\n",
      "\n",
      "SVD TF-IDF:\n",
      "[ 0.16403713 -0.06192932  0.00118506  0.05926234 -0.01728777 -0.0282153 ]\n",
      "(2, 100)\n",
      "\n",
      "SVD Hashing BOW:\n",
      "[ 0.51587004 -0.1754948   0.05378128  0.20697629 -0.04919744 -0.03881921]\n",
      "(2, 100)\n"
     ]
    }
   ],
   "source": [
    "example = dataset['train']['text'][:2]\n",
    "\n",
    "# Vecotrize document example\n",
    "bow_output = bow_vectorizer.transform(example)\n",
    "tfidf_output = tfidf_vectorizer.transform(bow_output)\n",
    "hashing_output = hashing_vectorizer.transform(example)\n",
    "svd_bow_output = svd_bow_vectorizer.transform(bow_output)\n",
    "svd_tfidf_output = svd_tfidf_vectorizer.transform(tfidf_output)\n",
    "svd_hashing_output = svd_hashing_vectorizer.transform(hashing_output)\n",
    "\n",
    "# Print document vectors and their shapes\n",
    "print('Document vector examples and their shapes:')\n",
    "print()\n",
    "print('BOW:')\n",
    "print(bow_output.todense())\n",
    "print(bow_output.shape)\n",
    "print()\n",
    "print('TF-IDF:')\n",
    "print(tfidf_output.todense())\n",
    "print(tfidf_output.shape)\n",
    "print()\n",
    "print('Hashing BOW:')\n",
    "print(hashing_output.todense())\n",
    "print(hashing_output.shape)\n",
    "print()\n",
    "print('SVD BOW:')\n",
    "print(svd_bow_output[0][:6])\n",
    "print(svd_bow_output.shape)\n",
    "print()\n",
    "print('SVD TF-IDF:')\n",
    "print(svd_tfidf_output[0][:6])\n",
    "print(svd_tfidf_output.shape)\n",
    "print()\n",
    "print('SVD Hashing BOW:')\n",
    "print(svd_hashing_output[0][:6])\n",
    "print(svd_hashing_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51485a-e469-4dba-8e1f-e49f62abde40",
   "metadata": {},
   "source": [
    "## Token vectorizers\n",
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873628bf-bf67-4713-a79f-910771d386f8",
   "metadata": {},
   "source": [
    "Train word2vec from the groud up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "beb38de3-5e15-401c-8e73-30b1e2662e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary shape:\n",
      "(10611, 100)\n",
      "\n",
      "Most frequent words:\n",
      "\"\n",
      "@user\n",
      "'\n",
      ",\n",
      "!\n",
      ".\n",
      ":\n",
      "...\n",
      "?\n",
      "may\n",
      "tomorrow\n",
      "go\n",
      ")\n",
      "day\n",
      "-\n",
      "get\n",
      "see\n",
      "like\n",
      "(\n",
      ";\n",
      "\n",
      "CPU times: user 6.08 ms, sys: 3.92 ms, total: 10 ms\n",
      "Wall time: 8.04 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vector_size = 100\n",
    "file = WORKING_PATH + 'word2vec_' + str(vector_size) + '.gensim'\n",
    "saving = True\n",
    "\n",
    "# Load model if it already exists\n",
    "if os.path.isfile(file):\n",
    "    word2vec = gensim.models.KeyedVectors.load(file, mmap='r')\n",
    "\n",
    "else:\n",
    "    # Initialize tokenizer wiht corpus in it\n",
    "    tokenizer = sentiment_utils.Tokenizer(dataset['train']['text']\n",
    "                                          + dataset['test']['text']\n",
    "                                          + dataset['validation']['text'])\n",
    "\n",
    "    # Train the model\n",
    "    word2vec = gensim.models.Word2Vec(\n",
    "        sentences=tokenizer, vector_size=vector_size, window=5, min_count=5, sg=1, hs=0, negative=5,\n",
    "        workers=7, epochs=5, seed=RANDOM_STATE,\n",
    "    )\n",
    "    \n",
    "    # Use the word vectors only\n",
    "    word2vec = word2vec.wv\n",
    "\n",
    "    # Save the model word vectors\n",
    "    if saving:\n",
    "        word2vec.save(file)\n",
    "\n",
    "# Print vocabulary shape\n",
    "print('Vocabulary shape:')\n",
    "print((len(word2vec.index_to_key), vector_size))\n",
    "print()\n",
    "\n",
    "# Print most frequent words\n",
    "print('Most frequent words:')\n",
    "for word in word2vec.index_to_key[:20]:\n",
    "    print(word)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20dec7-403d-4e23-b2eb-2ebda1ca5984",
   "metadata": {},
   "source": [
    "Show examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b4a724d-eae0-4972-9e91-362ce7271af4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A vector example:\n",
      "[-6.7988336e-03 -7.7007711e-03 -6.7419447e-03  7.7721477e-03\n",
      " -9.1446610e-03 -6.6873073e-03 -6.6153635e-03 -2.2669220e-03\n",
      "  5.0509833e-03  5.8403742e-03  6.4396439e-03  8.6656129e-03\n",
      " -8.7526087e-03 -9.2006801e-04 -1.6529012e-03 -6.5322830e-03\n",
      " -3.4659612e-03 -1.9954813e-03  8.2546510e-03  1.9973540e-03\n",
      " -9.0243109e-03  4.0886807e-03 -5.3359149e-04 -2.5054060e-03\n",
      " -6.9734524e-03 -4.2239283e-03 -1.2363232e-03  1.5906275e-03\n",
      "  1.5835894e-03  6.6484306e-03 -1.8646896e-03  9.8702870e-03\n",
      "  9.3534179e-03 -8.1601581e-03 -3.8998926e-03 -6.2233713e-03\n",
      " -3.3651828e-04  2.3092914e-03 -2.8936565e-03 -3.0549956e-03\n",
      "  3.3477665e-04 -2.8081452e-03 -7.9259863e-03 -8.3585903e-03\n",
      "  6.7217945e-04  9.0850675e-03 -8.8485815e-03 -3.2784594e-03\n",
      " -1.6568815e-03  7.9573207e-03  2.2853673e-03 -1.6162921e-03\n",
      " -7.9821423e-03  3.6615168e-03 -2.7477740e-06  2.6824963e-03\n",
      " -9.2297187e-03 -8.0831572e-03  2.4737692e-03  4.3313741e-03\n",
      " -6.3958620e-03 -1.2299264e-03  1.1683321e-03  9.0518082e-03\n",
      "  3.1548619e-04  3.0346382e-03  4.2372416e-03  6.7232894e-03\n",
      " -3.3694482e-03  1.9274366e-03  5.5399071e-03  2.0930672e-03\n",
      " -1.5624798e-03  5.8541680e-03 -6.7653833e-03  5.1936209e-03\n",
      "  2.0783448e-03  2.3969626e-03 -1.6580462e-04  6.9747807e-04\n",
      " -5.3472710e-03  1.8660760e-03 -6.5726280e-04  5.8715190e-03\n",
      "  7.2001112e-03  1.9464636e-03 -7.0486926e-03 -2.4714721e-03\n",
      "  8.2157822e-03  4.9938383e-03 -9.0842601e-03 -7.7735009e-03\n",
      "  2.8241635e-03 -4.6629561e-03 -2.8908777e-03  1.8842184e-03\n",
      "  5.3190198e-03  5.8819582e-03 -2.7071154e-03  4.0055071e-03]\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Print the vector example\n",
    "print('A vector example:')\n",
    "print(word2vec['@user'])\n",
    "print(word2vec['@user'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee6bb43-a899-4c87-bdbb-13b4cc7c53bc",
   "metadata": {},
   "source": [
    "### fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafac82e-ae86-4e39-857d-3573ec657921",
   "metadata": {},
   "source": [
    "Train fastTest from the groud up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7469688a-3d07-4084-a22f-cd8123778a88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary shape:\n",
      "(10611, 100)\n",
      "\n",
      "Most frequent words:\n",
      "\"\n",
      "@user\n",
      "'\n",
      ",\n",
      "!\n",
      ".\n",
      ":\n",
      "...\n",
      "?\n",
      "may\n",
      "tomorrow\n",
      "go\n",
      ")\n",
      "day\n",
      "-\n",
      "get\n",
      "see\n",
      "like\n",
      "(\n",
      ";\n",
      "\n",
      "CPU times: user 389 ms, sys: 295 ms, total: 684 ms\n",
      "Wall time: 3.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vector_size = 100\n",
    "file = WORKING_PATH + 'fasttext_' + str(vector_size) + '.gensim'\n",
    "saving = True\n",
    "\n",
    "# Load model if it already exists\n",
    "if os.path.isfile(file):\n",
    "    fasttext = gensim.models.fasttext.FastTextKeyedVectors.load(file, mmap='r')\n",
    "\n",
    "else:\n",
    "    # Initialize tokenizer wiht corpus in it\n",
    "    tokenizer = sentiment_utils.Tokenizer(dataset['train']['text']\n",
    "                                          + dataset['test']['text']\n",
    "                                          + dataset['validation']['text'])\n",
    "\n",
    "    # Train the model\n",
    "    fasttext = gensim.models.fasttext.FastText(\n",
    "        sentences=tokenizer, vector_size=vector_size, window=5, min_count=5, sg=1, hs=0, negative=5,\n",
    "        workers=7, epochs=5, seed=RANDOM_STATE,\n",
    "    )\n",
    "    \n",
    "    # Use the word vectors only\n",
    "    fasttext = fasttext.wv\n",
    "\n",
    "    # Save the model word vectors\n",
    "    if saving:\n",
    "        fasttext.save(file)\n",
    "\n",
    "# Print vocabulary shape\n",
    "print('Vocabulary shape:')\n",
    "print((len(fasttext.index_to_key), vector_size))\n",
    "print()\n",
    "\n",
    "# Print most frequent words\n",
    "print('Most frequent words:')\n",
    "for word in fasttext.index_to_key[:20]:\n",
    "    print(word)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed2c73-7434-47f7-8091-c71a15bec868",
   "metadata": {},
   "source": [
    "Show examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09f0bd21-0d25-44c9-9f5c-54adb58eed47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A vector example:\n",
      "[-4.87358047e-04  4.51807398e-04  8.77588638e-04 -3.32686584e-04\n",
      " -1.02142920e-03 -5.65615657e-04 -1.85830169e-03 -1.64301752e-03\n",
      " -3.25506413e-03  4.57338197e-03  1.27422193e-03  3.48202884e-03\n",
      "  1.94513152e-04  1.38777623e-05  2.34347157e-04  1.86236299e-04\n",
      "  1.52681104e-03  1.45538780e-03 -2.91749515e-04 -8.49718112e-04\n",
      " -1.08485331e-03  7.60952767e-04  2.18979130e-03  1.52153475e-03\n",
      " -8.63333640e-04 -1.12725690e-03 -4.13653994e-04 -9.69837129e-04\n",
      " -3.50951846e-03  2.00337311e-03  2.72217090e-03  9.62128979e-04\n",
      " -9.16386663e-04 -2.23301514e-03 -9.92241781e-04 -4.96376480e-04\n",
      " -2.19261716e-03  4.06168081e-04 -2.76850234e-03 -2.22463836e-03\n",
      "  1.32712605e-03 -9.71838774e-04 -3.71666916e-04 -3.45141103e-04\n",
      "  2.19832268e-03 -4.77933296e-04 -1.50938821e-03 -8.66060960e-04\n",
      "  1.31140207e-03 -1.85101863e-03  1.01888634e-03  2.18549496e-04\n",
      " -1.96930929e-03 -6.17635378e-04  2.47252802e-03 -8.30107136e-04\n",
      "  1.03148588e-04  3.78433871e-03 -1.67751324e-03  8.75202590e-04\n",
      " -7.45509460e-04 -7.74183136e-04 -1.29649055e-03 -3.31599877e-04\n",
      " -3.02510476e-03  2.44991155e-03 -1.57515251e-03  1.12388586e-03\n",
      " -2.91600311e-03 -4.86238365e-04  1.43003138e-03 -7.21334654e-04\n",
      "  2.63229012e-03  3.56963661e-04  1.17806520e-03 -8.68825533e-04\n",
      " -6.35157165e-04  8.25674972e-04  1.32847368e-03  2.76971678e-03\n",
      " -6.66425563e-04  4.56986832e-04  2.44077365e-03 -1.48916186e-03\n",
      "  2.13712151e-03 -8.01523274e-04 -8.83655041e-04 -4.45649057e-04\n",
      " -1.41387770e-03  1.21046486e-03 -1.74477522e-03  3.50933144e-04\n",
      " -9.27234869e-05 -1.16335781e-04 -2.03814032e-03  1.53253477e-05\n",
      "  8.33417071e-05  2.39008456e-03 -1.16614625e-04  1.16813590e-03]\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Print the vector example\n",
    "print('A vector example:')\n",
    "print(fasttext['@user'])\n",
    "print(fasttext['@user'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76379d4-e7d6-4381-a88d-d3fc6f9ef03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
