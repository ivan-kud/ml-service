{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c5c0ef-097e-4f17-b16d-e4e1466bee2f",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c592697-3b05-4365-86b3-944be160620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:          3.11.5 (main, Aug 24 2023, 15:09:45) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "Transformers:    4.33.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import transformers\n",
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "\n",
    "\n",
    "print('Python:'.ljust(16), sys.version.split('\\n')[0])\n",
    "print('Transformers:'.ljust(16), transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57f60a2-b1d6-41c9-9bc8-6099f531a3c7",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29455f3-a438-4dd2-9ed1-da4fe278f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = './suggestion-data/'\n",
    "\n",
    "# Pretrained model name (checkpoint)\n",
    "MODEL_NAME = 'roberta-base'\n",
    "MODEL_INTERNAL_DIM = 768\n",
    "\n",
    "# Number of suggestions\n",
    "SUGGESTION_NUM = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c2da4-9542-417f-9117-e37f9bd6a2a6",
   "metadata": {},
   "source": [
    "# Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c866de09-2865-41f7-88b0-2ee409cd9023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model = RobertaModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7deaf-8fcb-4430-8df2-98038876d304",
   "metadata": {},
   "source": [
    "# Read terms ans sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bef44ffe-a7a6-4731-a660-a2014d2f9cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' optimal performance', ' utilise resources', ' enhance productivity', ' conduct an analysis', ' maintain a high standard', ' implement best practices', ' ensure compliance', ' streamline operations', ' foster innovation', ' drive growth', ' leverage synergies', ' demonstrate leadership', ' exercise due diligence', ' maximize stakeholder value', ' prioritise tasks', ' facilitate collaboration', ' monitor performance metrics', ' execute strategies', ' gauge effectiveness', ' champion change']\n",
      "\n",
      "In today's meeting, we discussed a variety of issues affecting our department. The weather was unusually sunny, a pleasant backdrop to our serious discussions. We came to the consensus that we need to do better in terms of performance. Sally brought doughnuts, which lightened the mood. It's important to make good use of what we have at our disposal. During the coffee break, we talked about the upcoming company picnic. We should aim to be more efficient and look for ways to be more creative in our daily tasks. Growth is essential for our future, but equally important is building strong relationships with our team members. As a reminder, the annual staff survey is due next Friday. Lastly, we agreed that we must take time to look over our plans carefully and consider all angles before moving forward. On a side note, David mentioned that his cat is recovering well from surgery.\n"
     ]
    }
   ],
   "source": [
    "# Read terms\n",
    "with open(DATA_PATH + 'terms.csv') as f:\n",
    "    terms = f.read()\n",
    "\n",
    "# Preprocess terms\n",
    "terms = terms.lower().split('\\n')\n",
    "terms = [' ' + term for term in terms]\n",
    "\n",
    "# Read sample text\n",
    "with open(DATA_PATH + 'sample-text.txt') as f:\n",
    "    sample = f.read()\n",
    "    \n",
    "# Print terms and sample text\n",
    "print(terms)\n",
    "print()\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ae4c0-af46-4244-bfb5-44d8cc0985d0",
   "metadata": {},
   "source": [
    "# Get embeddings of the terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "123e242e-4314-4bb9-82cf-22a353d0054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_state(text):\n",
    "    # Get offset mapping\n",
    "    model_input = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        return_special_tokens_mask=True,\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "    offset_mapping = model_input['offset_mapping'].detach().numpy().squeeze()\n",
    "\n",
    "    # Get last hidden state\n",
    "    model_output = model(\n",
    "        input_ids=model_input['input_ids'],\n",
    "        attention_mask=model_input['attention_mask'],\n",
    "    )\n",
    "    last_hidden_state = model_output['last_hidden_state'].detach().numpy().squeeze()\n",
    "    \n",
    "    # Mask to get no special tokens\n",
    "    mask = model_input['special_tokens_mask'].detach().numpy().squeeze() == 0\n",
    "    last_hidden_state = last_hidden_state[mask, :]\n",
    "    offset_mapping = offset_mapping[mask, :]\n",
    "\n",
    "    return last_hidden_state, offset_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eba56b00-7335-44db-b6f4-03eeb820b821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 768)\n"
     ]
    }
   ],
   "source": [
    "# Compute mean pooling embedding\n",
    "term_embedding = np.empty((len(terms), MODEL_INTERNAL_DIM))\n",
    "for i, term in enumerate(terms):\n",
    "    last_hidden_state, _ = get_hidden_state(term)\n",
    "    term_embedding[i] = np.sum(last_hidden_state, axis=0) / last_hidden_state.shape[0]\n",
    "\n",
    "# Print shape\n",
    "print(term_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f514b-c2be-4188-bd06-e49f87f1786a",
   "metadata": {},
   "source": [
    "# Get embeddings of the sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a9144a2-63ac-4e34-80e4-ec5137f7d170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1372, 768)\n",
      "(1372, 2)\n"
     ]
    }
   ],
   "source": [
    "# get last_hidden_state and offset_mapping\n",
    "last_hidden_state, offset_mapping = get_hidden_state(sample)\n",
    "\n",
    "# Compute number of contexts (array length)\n",
    "length = 0\n",
    "for i in range(last_hidden_state.shape[0]):\n",
    "    for j in range(i, i+8):\n",
    "        if j >= last_hidden_state.shape[0]:\n",
    "            continue\n",
    "        length += 1\n",
    "\n",
    "# Compute mean pooling embeddings for all contexts in sample text\n",
    "context_embedding = np.empty((length, MODEL_INTERNAL_DIM))\n",
    "context_mapping = np.empty((length, 2), int)\n",
    "index = 0\n",
    "for i in range(last_hidden_state.shape[0]):\n",
    "    for j in range(i, i+8):\n",
    "        if j < last_hidden_state.shape[0]:\n",
    "            slice = last_hidden_state[i:j+1, :]\n",
    "            context_embedding[index] = np.sum(slice, axis=0) / slice.shape[0]\n",
    "            context_mapping[index] = np.array((offset_mapping[i, 0], offset_mapping[j, 1]))\n",
    "            index += 1\n",
    "\n",
    "# Print shapes\n",
    "print(context_embedding.shape)\n",
    "print(context_mapping.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef23deff-d44b-4aef-baf9-5b704843c996",
   "metadata": {},
   "source": [
    "# Get suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00737410-6d99-4f15-8f93-0725340794c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53, 78]\n",
      "affecting our department.\n",
      "monitor performance metrics\n",
      "0.9237443330959721\n",
      "\n",
      "[139, 159]\n",
      "serious discussions.\n",
      "monitor performance metrics\n",
      "0.9220517883443872\n",
      "\n",
      "[406, 421]\n",
      "company picnic.\n",
      "monitor performance metrics\n",
      "0.9208585375958583\n",
      "\n",
      "[281, 291]\n",
      "mood. It's\n",
      "monitor performance metrics\n",
      "0.9111545349126645\n",
      "\n",
      "[220, 241]\n",
      "of performance. Sally\n",
      "monitor performance metrics\n",
      "0.909594951782594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity and sort\n",
    "if context_embedding.shape[0]:\n",
    "    similarity = cosine_similarity(context_embedding, term_embedding)\n",
    "else:\n",
    "    similarity = np.array([[]])\n",
    "flat_indices = np.flip(np.argsort(similarity, axis=None))\n",
    "\n",
    "# Get suggestions\n",
    "spans = []\n",
    "original_phrases = []\n",
    "replacements = []\n",
    "scores = []\n",
    "for index in flat_indices:\n",
    "    row = index // len(terms)\n",
    "    col = index % len(terms)\n",
    "\n",
    "    # Get current suggestion in the order\n",
    "    text_span = context_mapping[row].tolist()\n",
    "    original_phrase = sample[text_span[0]:text_span[1]]\n",
    "    replacement = terms[col][1:]\n",
    "    score = similarity[row, col].item()\n",
    "\n",
    "    # Check if it is a new span\n",
    "    new_span = True\n",
    "    for span in spans:\n",
    "        if ((span[0] <= text_span[0] < span[1])\n",
    "            or (span[0] < text_span[1] <= span[1])\n",
    "            or (text_span[0] < span[0] and span[1] < text_span[1])):\n",
    "            new_span = False\n",
    "\n",
    "    # Add suggestion if it is a new span\n",
    "    if new_span:\n",
    "        spans.append(text_span)\n",
    "        original_phrases.append(original_phrase)\n",
    "        replacements.append(replacement)\n",
    "        scores.append(score)\n",
    "        \n",
    "        if len(spans) >= SUGGESTION_NUM:\n",
    "            break\n",
    "\n",
    "# Print result\n",
    "for i in range(len(spans)):\n",
    "    print(spans[i])\n",
    "    print(original_phrases[i])\n",
    "    print(replacements[i])\n",
    "    print(scores[i])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
